---
title: "Оценка ИИ в dbt"
description: "Как расширить тестирование качества в dbt, чтобы мониторить агентное качество ИИ"
slug: ai-eval-in-dbt

authors: [kyle_dempsey, luis_leon]

tags: [analytics craft]
hide_table_of_contents: false

date: 2025-05-04
is_featured: true
---
**Революция AI уже здесь — но готовы ли мы?**  
По всему миру ажиотаж вокруг AI трудно не заметить. Повсеместно обсуждают большие языковые модели, агентные workflow и то, как AI изменит каждую отрасль. Однако реальных примеров использования AI в продакшене по‑прежнему немного.

Одной из ключевых проблем, мешающих переводу AI‑кейсов в продакшен, является отсутствие возможности систематически и управляемо оценивать корректность ответов AI.
Переход от прототипов к продакшену требует строгой оценки, и у большинства организаций нет фреймворка, который гарантировал бы, что AI‑результаты остаются качественными, надёжными и пригодными для принятия решений.

<!-- truncate -->

**Почему оценка AI так важна**  
Чем больше мы общаемся с командами данных, тем яснее становится проблема: компании не готовы выводить AI в продакшен, пока не смогут контролировать и гарантировать его качество после запуска — страх перед «вышедшим из‑под контроля AI» всё ещё перевешивает потенциальную выгоду.

Основная сложность заключается не только в создании AI‑кейсов, но и в постоянном мониторинге их работы и обеспечении того же уровня качества и надёжности, к которому мы привыкли для других дата‑активов.
Чтобы доверять AI в продакшене, нам нужны структурированные workflow, которые:
- **Гарантируют качество данных** до того, как они попадут в AI‑модели  
- **Оценивают ответы, сгенерированные AI**, по сравнению с эталонными, заведомо корректными ответами  
- **Запускают алерты или корректирующие действия**, когда качество работы AI опускается ниже допустимых порогов  

Без этих возможностей AI‑workflow остаются на экспериментальной стадии и не соответствуют требованиям надёжности, необходимым для продакшена.

**Использование dbt для построения workflow оценки AI**  
Большинство организаций уже используют dbt для трансформации, тестирования и валидации данных.
Поскольку dbt уже зарекомендовал себя как надёжный фреймворк для обеспечения качества данных, логичным шагом стало использование его тестовых возможностей для оценки и мониторинга AI‑workflow.

Рассмотрим простой пример использования **dbt и Snowflake Cortex** для оценки AI.
- **Ingest Data** Мы начинаем с загрузки датасета с отзывами о фильмах из IMDB вместе с размеченной человеком тональностью (положительная или отрицательная). Эти данные служат источником истины.  
- **Run AI Workflow** В качестве простого примера workflow мы используем функцию анализа тональности Snowflake Cortex для классификации каждого отзыва.  
- **Evaluate AI Output versus Human Review** Мы создаём модель оценки в dbt, которая с помощью функции Cortex Complete сравнивает сгенерированную AI тональность с фактической тональностью, заданной человеком.  
- **Define Pass/Fail Criteria** Мы настраиваем кастомный dbt‑тест, задавая порог точности (например, 75%). Если точность предсказаний тональности падает ниже этого уровня, тест вызывает предупреждение или ошибку.  
- **Store and Visualize Results** Встроенные возможности dbt позволяют сохранять результаты падений тестов в хранилище, обеспечивая трассируемость для дальнейшего анализа, а также данные для отчётности по точности AI.

**Масштабирование оценки AI с помощью dbt**  
Этот workflow естественным образом расширяет нативные возможности тестирования dbt и использует мощную возможность встраивать вызовы Snowflake Cortex прямо в SQL‑модели.
Таким образом пользователи могут объединить мощь Snowflake Cortex с устоявшейся системой управления и контроля качества dbt для решения описанных выше задач.

Используя dbt для оценки AI, организации могут применять те же строгие принципы тестирования, которые они уже используют для data‑pipeline, чтобы гарантировать готовность AI‑моделей к продакшену и централизованно поддерживать качество и управление всеми дата‑активами.

**Что мы построили**  
Давайте разберём этот пример шаг за шагом, чтобы понять, как всё работает.
В качестве отправной точки мы используем тестовый датасет, который содержит входные данные для AI‑workflow, а также эталонное значение, заданное человеком. В нашем примере входными данными является текст отзыва о фильме, а поле `actual_sentiment` содержит значение -1 для негативных отзывов и 1 для позитивных.
Также мы добавляем временную метку, указывающую, когда AI выдал свой ответ. Эта метка позволяет отслеживать точность AI с течением времени.

<Lightbox src="/img/blog/2025-04-04-ai-evaluation-and-how-dbt-can-help/ai_eval_blog_image_one.png" title="our input data set, including actual sentiment"width="85%" />

Следующий шаг — создание выходной таблицы, содержащей как эталонное значение из датасета, так и значение, возвращённое AI.
Поскольку вызов Snowflake Cortex можно напрямую встроить в SQL‑модель, мы легко реализуем это в dbt, используя простую reference‑функцию.

<Lightbox src="/img/blog/2025-04-04-ai-evaluation-and-how-dbt-can-help/ai_eval_blog_image_two.png" title="results of our agentic workflow"width="85%" />

Мы также включаем входные данные для AI‑workflow вместе со значениями, рассчитанными AI и определёнными человеком.
Хотя включение всех этих данных не является строго обязательным, оно позволяет чётко понимать, что именно было подано на вход AI‑workflow, и обеспечивает удобную трассируемость конкретных ответов.
Мы повторяем этот же паттерн, используя reference‑функцию dbt, чтобы создать ещё одну модель dbt, в которой формируем evaluation‑prompt и используем Cortex Complete для передачи этого prompt в Cortex с сохранением результатов.
Основная часть работы при создании этой модели заключалась в prompt engineering для evaluation‑prompt. Изначально мы создавали prompt напрямую в Snowflake Cortex, чтобы убедиться, что он возвращает нужный тип ответа, прежде чем переносить его в dbt.

<Lightbox src="/img/blog/2025-04-04-ai-evaluation-and-how-dbt-can-help/ai_eval_blog_image_three.png" title="AI generated results automatically evaluated by one or more models"width="85%" />

Мы решили определить prompt как Jinja‑переменную, а не прописывать его напрямую в каждой модели dbt.
Это повышает читаемость модели, но при этом скрывает текст prompt от тех, кто просто читает модель.
Чтобы решить эту проблему и обеспечить полную трассируемость, мы материализуем prompt в виде колонки в таблице. Это означает, что каждая строка результата содержит не только оценку, но и точный prompt, который был использован для её получения.
Независимо от того, где вы определяете evaluation‑prompt, включение его в dbt‑проект позволяет применять к нему те же процессы управления изменениями и контроля версий, что и ко всему остальному проекту dbt, обеспечивая надёжное управление AI‑workflow.
Ещё одно важное преимущество этого подхода и гибкости dbt и Snowflake Cortex заключается в том, что вы можете легко переключать модель, используемую для оценки. В этом примере мы используем Snowflake Llama, но переход на любую другую [поддерживаемую модель](https://docs.snowflake.com/en/sql-reference/functions/complete-snowflake-cortex) сводится к изменению одного параметра функции.
Вы также можете запускать несколько оценок с использованием разных моделей, просто добавив дополнительные колонки в модель dbt.

<Lightbox src="/img/blog/2025-04-04-ai-evaluation-and-how-dbt-can-help/ai_eval_blog_image_four.png" title="dbt Testing evaluates AI accuracry along side data quality"width="85%" />

Финальный шаг — написание dbt [custom test](/best-practices/writing-custom-generic-tests), который находит ответы, не соответствующие заданному порогу точности. Создав такой dbt‑тест, мы можем гарантировать, что проблемы с точностью AI будут обнаружены и помечены в рамках стандартных запусков dbt и проверок качества.
Мы также можем легко использовать возможность dbt [store test failures](/reference/resource-configs/store_failures), чтобы сохранять найденные проблемы качества в AI‑процессах для дальнейшего анализа и обработки.

В качестве дополнительного преимущества фиксации результатов оценки AI в dbt‑проекте можно отметить следующее: информация о качестве AI становится частью dbt‑проекта, а значит результаты доступны всеми теми же способами, что и любые другие результаты dbt‑тестов.
Вы можете просматривать эту информацию в <Constant name="explorer" />, передавать её в используемый вами data catalog, применять результаты тестов для запуска дополнительных downstream‑процессов или визуализировать их в BI‑дашбордах качества.
По мере того как AI‑workflow становятся всё более распространёнными, бизнесу необходим системный подход к оценке и мониторингу AI‑результатов — так же, как и для традиционных data‑продуктов. К счастью, те же принципы и инструменты dbt легко применимы и к оценке AI.
С dbt команды данных могут преодолеть разрыв между AI‑экспериментами и AI в продакшене, обеспечивая доверие, надёжность и управляемость AI‑workflow.

Готовы внедрить оценку AI в ваш dbt‑workflow? Начните с dbt MCP server — он упрощает подключение AI‑систем к надёжным и управляемым данным.
