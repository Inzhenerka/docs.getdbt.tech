---
title: Перевод вашего проекта dbt Databricks в продакшн
id: productionize-your-dbt-databricks-project
description: "Узнайте, как доставлять модели конечным пользователям и использовать лучшие практики для поддержания данных в продакшене." 
displayText: Перевод вашего проекта dbt Databricks в продакшн
hoverSnippet: Узнайте, как перевести ваш проект dbt Databricks в продакшн.
icon: 'databricks'
hide_table_of_contents: true
tags: ['Databricks','dbt Core','dbt Cloud']
level: 'Intermediate'
recently_updated: true
---

<div style={{maxWidth: '900px'}}>

## Введение

Добро пожаловать в третью часть нашей обширной серии по оптимизации и развертыванию ваших конвейеров данных с использованием Databricks и dbt Cloud. В этом руководстве мы углубимся в доставку этих моделей конечным пользователям, одновременно внедряя лучшие практики, чтобы ваши данные в продакшене оставались надежными и своевременными.

### Предварительные требования

Если у вас нет каких-либо из следующих требований, обратитесь к инструкциям в [Настройка вашего проекта dbt с Databricks](/guides/set-up-your-databricks-dbt-project) для помощи в их выполнении:

- Вы настроили [ваш проект dbt с Databricks](/guides/set-up-your-databricks-dbt-project).
- Вы [оптимизировали ваши модели dbt для максимальной производительности](/guides/optimize-dbt-models-on-databricks).
- Вы создали два каталога в Databricks: *dev* и *prod*.
- Вы создали Databricks Service Principal для выполнения ваших продакшн задач.
- У вас есть как минимум одна [среда развертывания](/docs/deploy/deploy-environments) в dbt Cloud.

Чтобы начать, давайте вернемся к среде развертывания, созданной для ваших данных в продакшене.

### Среды развертывания

В программной инженерии среды играют ключевую роль, позволяя инженерам разрабатывать и тестировать код, не влияя на конечных пользователей их программного обеспечения. Аналогично, вы можете проектировать [data lakehouses](https://www.databricks.com/product/data-lakehouse) с отдельными средами. _Продакшн_ среда включает в себя отношения (схемы, таблицы и представления), которые запрашивают или используют конечные пользователи, обычно в BI инструменте или модели ML.

В dbt Cloud [среды](/docs/dbt-cloud-environments) бывают двух видов:

- Развертывание &mdash; Определяет настройки, используемые для выполнения задач, созданных в этой среде.
- Разработка &mdash; Определяет настройки, используемые в IDE dbt Cloud для конкретного проекта dbt Cloud.

Каждый проект dbt Cloud может иметь несколько сред развертывания, но только одну среду разработки на пользователя.

## Создание и планирование продакшн задачи

С вашей настроенной средой развертывания пришло время создать продакшн задачу для выполнения в вашей *prod* среде.

Для развертывания наших рабочих процессов трансформации данных мы будем использовать [встроенный планировщик задач dbt Cloud](/docs/deploy/deploy-jobs). Планировщик задач специально разработан для упрощения развертывания и выполнения вашего проекта dbt, обеспечивая легкость создания, мониторинга и модификации ваших конвейеров данных.

Использование планировщика задач dbt Cloud позволяет командам данных владеть всем рабочим процессом трансформации. Вам не нужно изучать и поддерживать дополнительные инструменты для оркестрации или полагаться на другую команду для планирования кода, написанного вашей командой. Это полное владение упрощает процесс развертывания и ускоряет доставку новых продуктов данных.

Давайте [создадим задачу](/docs/deploy/deploy-jobs#create-and-schedule-jobs) в dbt Cloud, которая будет трансформировать данные в нашем каталоге Databricks *prod*.

1. Создайте новую задачу, нажав **Deploy** в заголовке, затем **Jobs** и **Create job**.
2. **Назовите** задачу “Ежедневное обновление”.
3. Установите **Environment** на вашу *production* среду.
    - Это позволит задаче наследовать каталог, схему, учетные данные и переменные среды, определенные в [Настройка вашего проекта dbt с Databricks](/guides/set-up-your-databricks-dbt-project).
4. В разделе **Execution Settings**
    - Установите флажок **Generate docs on run**, чтобы настроить задачу на автоматическую генерацию документации проекта каждый раз, когда эта задача выполняется. Это обеспечит актуальность вашей документации по мере добавления и изменения моделей.
    - Выберите флажок **Run on source freshness**, чтобы настроить dbt [source freshness](/docs/deploy/source-freshness) как первый шаг этой задачи. Ваши источники должны быть настроены на [снимок информации о свежести](/docs/build/sources#source-data-freshness), чтобы это давало значимые инсайты.

    Добавьте следующие три **Commands:**
    - `dbt source freshness`
        - Это проверит, не устарели ли какие-либо источники. Мы не хотим пересчитывать модели с данными, которые не изменились с момента нашего последнего запуска.
    - `dbt test --models source:*`
        - Это проверит качество данных наших исходных данных, например, убедится, что поля ID уникальны и не пусты. Мы не хотим, чтобы плохие данные попадали в продакшн модели.
    - `dbt build --exclude source:* --fail-fast`
        - dbt build более эффективен, чем выполнение отдельных команд для dbt run и dbt test по отдельности, потому что он сначала выполняет, а затем тестирует каждую модель перед продолжением.
        - Мы исключаем исходные данные, потому что уже протестировали их на шаге 2.
        - Флаг fail-fast заставит dbt немедленно выйти, если один ресурс не удастся построить. Если другие модели находятся в процессе выполнения, когда первая модель не удается, dbt завершит соединения для этих все еще выполняющихся моделей.
5. В разделе **Triggers** используйте переключатель, чтобы настроить вашу задачу на [выполнение по расписанию](/docs/deploy/deploy-jobs#schedule-days). Вы можете ввести конкретные дни и время или создать пользовательский cron-расписание.
    - Если вы хотите, чтобы ваша задача dbt Cloud была запланирована другим оркестратором, например, Databricks Workflows, см. раздел [Advanced Considerations](#advanced-considerations) ниже.

Это всего лишь один пример списка команд "все или ничего", предназначенного для минимизации потерь вычислительных ресурсов. [Список команд задач](/docs/deploy/job-commands) и [селекторы](/reference/node-selection/syntax) предоставляют большую гибкость в том, как будет выполняться ваш DAG. Вы можете захотеть спроектировать свой так, чтобы продолжать выполнение определенных моделей, если другие не удаются. Вы можете захотеть настроить несколько задач для обновления моделей с разной частотой. См. наш [Job Creation Best Practices discourse](https://discourse.getdbt.com/t/job-creation-best-practices-in-dbt-cloud-feat-my-moms-lasagna/2980) для получения дополнительных предложений по проектированию задач.

После того как ваша задача настроена и успешно выполняется, настройте ваши **[артефакты проекта](/docs/deploy/artifacts)**, чтобы эта задача информировала ваш сайт документации продакшн и панель данных источников, к которым можно получить доступ из пользовательского интерфейса.

Это будет наша основная продакшн задача для обновления данных, которые будут использоваться конечными пользователями. Еще одна задача, которую все должны включить в свой проект dbt, это задача непрерывной интеграции.

## Добавление задачи CI

CI/CD, или Непрерывная Интеграция и Непрерывное Развертывание/Доставка, стала стандартной практикой в разработке программного обеспечения для быстрого предоставления новых функций и исправлений ошибок при поддержании высокого качества и стабильности. dbt Cloud позволяет вам применять эти практики к вашим трансформациям данных.

Шаги ниже показывают, как создать тест CI для вашего проекта dbt. CD в dbt Cloud не требует дополнительных шагов, так как ваши задачи автоматически подхватят последние изменения из ветки, назначенной среде, в которой выполняется ваша задача. Вы можете выбрать добавление шагов в зависимости от вашей стратегии развертывания. Если вы хотите углубиться в варианты CD, ознакомьтесь с [этим блогом о внедрении CI/CD с dbt Cloud](https://www.getdbt.com/blog/adopting-ci-cd-with-dbt-cloud/).

dbt позволяет вам писать [тесты](/docs/build/data-tests) для вашего конвейера данных, которые могут выполняться на каждом этапе процесса, чтобы обеспечить стабильность и правильность ваших трансформаций данных. Основные места, где вы будете использовать ваши тесты dbt:

1. **Ежедневные запуски:** Регулярное выполнение тестов на вашем конвейере данных помогает выявлять проблемы, вызванные плохими исходными данными, обеспечивая качество данных, которые достигают ваших пользователей.
2. **Разработка**: Выполнение тестов во время разработки гарантирует, что изменения в вашем коде не нарушают существующие предположения, позволяя разработчикам быстрее итеративно работать, выявляя проблемы сразу после написания кода.
3. **Проверки CI**: Автоматизированные задачи CI выполняют и тестируют ваш конвейер от начала до конца, когда создается запрос на слияние, обеспечивая уверенность разработчикам, рецензентам кода и конечным пользователям, что предлагаемые изменения надежны и не вызовут сбоев или проблем с качеством данных.

Ваша задача CI будет гарантировать, что модели правильно строятся и проходят любые примененные к ним тесты. Мы рекомендуем создать отдельную *тестовую* среду и иметь выделенный сервисный принципал. Это обеспечит, что временные схемы, создаваемые во время тестов CI, находятся в своем собственном каталоге и не могут случайно раскрыть данные другим пользователям. Повторите шаги в [Настройка вашего проекта dbt с Databricks](/guides/set-up-your-databricks-dbt-project) для создания вашей *prod* среды, чтобы создать *тестовую* среду. После настройки у вас должно быть:

- Каталог под названием *test*
- Сервисный принципал под названием *dbt_test_sp*
- Новая среда dbt Cloud под названием *test*, которая по умолчанию использует каталог *test* и использует токен *dbt_test_sp* в учетных данных развертывания

Мы рекомендуем настроить задачу CI в dbt Cloud. Это уменьшит время выполнения задачи, выполняя и тестируя только измененные модели, что также снижает затраты на вычисления в lakehouse. Чтобы создать задачу CI, обратитесь к [Настройка задач CI](/docs/deploy/ci-jobs) для получения подробной информации.

С тестами dbt и SlimCI вы можете быть уверены, что ваши данные в продакшене будут своевременными и точными, даже при высокой скорости доставки.

## Мониторинг ваших задач

Внимательное наблюдение за вашими задачами dbt Cloud имеет решающее значение для поддержания надежного и эффективного конвейера данных. Мониторинг производительности задач и быстрое выявление потенциальных проблем позволяют гарантировать, что ваши трансформации данных выполняются гладко. dbt Cloud предоставляет три точки входа для мониторинга состояния вашего проекта: история запусков, монитор развертывания и статусные плитки.

Панель [истории запусков](/docs/deploy/run-visibility#run-history) в dbt Cloud предоставляет детальный обзор всех запусков задач вашего проекта, предлагая различные фильтры, чтобы помочь вам сосредоточиться на конкретных аспектах. Это отличный инструмент для разработчиков, которые хотят проверить недавние запуски, подтвердить результаты ночных запусков или отслеживать прогресс выполняющихся задач. Чтобы получить доступ, выберите **Run History** в меню **Deploy**.

Монитор развертывания в dbt Cloud предлагает более высокий уровень обзора вашей истории запусков, позволяя вам оценить состояние вашего конвейера данных за длительный период времени. Эта функция включает информацию о продолжительности запусков и уровнях успеха, позволяя вам выявлять тенденции в производительности задач, такие как увеличение времени выполнения или более частые сбои. Монитор развертывания также выделяет задачи в процессе выполнения, в очереди и недавние сбои. Чтобы получить доступ к монитору развертывания, нажмите на логотип dbt в верхнем левом углу интерфейса dbt Cloud.

<Lightbox src="/img/guides/databricks-guides/deployment_monitor_dbx.png" width="85%" title="Монитор развертывания показывает статус задач с течением времени в разных средах" />

Добавляя [плитки состояния данных](/docs/collaborate/data-tile) на ваши BI панели, вы можете предоставить заинтересованным сторонам видимость состояния вашего конвейера данных, не покидая их предпочтительный интерфейс. Плитки данных внушают уверенность в ваших данных и помогают предотвратить ненужные запросы или переключение контекста. Чтобы реализовать плитки состояния панели, вам нужно иметь dbt docs с определенными [exposures](/docs/build/exposures).

## Настройка уведомлений

Настройка [уведомлений](/docs/deploy/job-notifications) в dbt Cloud позволяет вам получать оповещения по электронной почте или в канале Slack каждый раз, когда запуск заканчивается. Это гарантирует, что соответствующие команды будут уведомлены и смогут быстро принять меры, когда задачи не удаются или отменяются. Чтобы настроить уведомления:

1. Перейдите к настройкам вашего проекта dbt Cloud.
2. Выберите вкладку **Notifications**.
3. Выберите желаемый тип уведомления (Email или Slack) и настройте соответствующие параметры.

Если вам требуются уведомления другими средствами, кроме электронной почты или Slack, вы можете использовать функцию исходящих [вебхуков](/docs/deploy/webhooks) dbt Cloud для передачи событий задач в другие инструменты. Вебхуки позволяют интегрировать dbt Cloud с широким спектром SaaS приложений, расширяя автоматизацию вашего конвейера в другие системы.

## Устранение неполадок

Когда в вашем продакшн конвейере происходит сбой, важно знать, как эффективно устранять проблемы, чтобы минимизировать время простоя и поддерживать высокий уровень доверия со стороны ваших заинтересованных сторон.

Пять ключевых шагов для устранения неполадок в dbt Cloud:

1. Прочитайте сообщение об ошибке: сообщения об ошибках dbt обычно указывают тип ошибки и файл, в котором произошла проблема.
2. Проверьте проблемный файл и найдите немедленное решение.
3. Изолируйте проблему, выполняя одну модель за раз в IDE или отменяя код, вызвавший проблему.
4. Проверьте проблемы в скомпилированных файлах и журналах.

Обратитесь к [документации по отладке ошибок](/guides/debug-errors) для получения полного списка типов ошибок и методов диагностики.

Чтобы устранить проблемы с задачей dbt Cloud, перейдите на вкладку "Deploy > Run History" в вашем проекте dbt Cloud и выберите неудавшийся запуск. Затем разверните шаги запуска, чтобы просмотреть [консольные и отладочные журналы](/docs/deploy/run-visibility#access-logs) для изучения подробных сообщений журнала. Чтобы получить дополнительную информацию, откройте вкладку Артефакты и загрузите скомпилированные файлы, связанные с запуском.

Если ваши задачи занимают больше времени, чем ожидалось, используйте панель [времени выполнения модели](/docs/deploy/run-visibility#model-timing) для выявления узких мест в вашем конвейере. Анализ времени, затраченного на выполнение каждой модели, помогает вам определить самые медленные компоненты и оптимизировать их для повышения производительности. История запросов Databricks [Query History](https://docs.databricks.com/sql/admin/query-history.html) позволяет вам изучать детальные сведения, такие как время, затраченное на каждую задачу, возвращенные строки, производительность ввода-вывода и план выполнения.

Для получения дополнительной информации о настройке производительности ознакомьтесь с нашим руководством по [Как оптимизировать и устранять неполадки моделей dbt на Databricks](/guides/optimize-dbt-models-on-databricks).

## Продвинутые соображения

По мере того, как вы становитесь более опытным в работе с dbt Cloud и Databricks, вы можете захотеть изучить продвинутые техники для дальнейшего улучшения вашего конвейера данных и улучшения управления вашими трансформациями данных. Темы в этом разделе не являются обязательными, но помогут вам укрепить вашу продакшн среду для повышения безопасности, эффективности и доступности.

### Обновление ваших данных с помощью Databricks Workflows

Планировщик задач dbt Cloud предлагает несколько способов запуска ваших задач. Если ваши трансформации dbt являются всего лишь одним шагом в более крупном оркестрационном рабочем процессе, используйте API dbt Cloud для запуска вашей задачи из Databricks Workflows.

Это распространенный шаблон для аналитических случаев использования, которые хотят минимизировать задержку между загрузкой бронзовых данных в lakehouse с помощью ноутбука, преобразованием этих данных в золотые таблицы с помощью dbt и обновлением панели. Это также полезно для команд по науке о данных, которые используют dbt для извлечения признаков перед использованием обновленного хранилища признаков для обучения и регистрации моделей машинного обучения с MLflow.

API обеспечивает интеграцию между вашими задачами dbt Cloud и рабочим процессом Databricks, гарантируя, что ваши трансформации данных эффективно управляются в более широком контексте вашего конвейера обработки данных.

Вставка задач dbt Cloud в Databricks Workflows позволяет вам связывать внешние задачи, при этом используя эти преимущества dbt Cloud:

- Контекст UI: Интерфейс dbt Cloud позволяет вам определять задачу в контексте ваших сред dbt Cloud, упрощая создание и управление соответствующими конфигурациями.
- Журналы и история запусков: Доступ к журналам и истории запусков становится более удобным при использовании dbt Cloud.
- Функции мониторинга и уведомлений: dbt Cloud оснащен функциями мониторинга и уведомлений, как описано выше, которые могут помочь вам оставаться в курсе статуса и производительности ваших задач.

Чтобы запустить вашу задачу dbt Cloud из Databricks, следуйте инструкциям в нашем [руководстве по использованию Databricks Workflows для запуска задач dbt Cloud](/guides/how-to-use-databricks-workflows-to-run-dbt-cloud-jobs).

## Маскирование данных

Наше руководство [Лучшие практики для dbt и Unity Catalog](/best-practices/dbt-unity-catalog-best-practices) рекомендует использовать отдельные каталоги *dev* и *prod* для сред разработки и развертывания, с Unity Catalog и dbt Cloud, управляющими конфигурациями и разрешениями для изоляции сред. Обеспечение безопасности при поддержании эффективности в ваших средах разработки и развертывания имеет решающее значение. Дополнительные меры безопасности могут быть необходимы для защиты конфиденциальных данных, таких как персонально идентифицируемая информация (PII).

Databricks использует [Динамические представления](https://docs.databricks.com/data-governance/unity-catalog/create-views.html#create-a-dynamic-view) для включения маскирования данных на основе членства в группе. Поскольку представления в Unity Catalog используют Spark SQL, вы можете реализовать продвинутое маскирование данных, используя более сложные SQL выражения и регулярные выражения. Вы также можете применять более детализированные средства управления доступом, такие как фильтры строк в предварительном просмотре и маски столбцов в предварительном просмотре на таблицах в Databricks Unity Catalog, что будет рекомендованным подходом для защиты конфиденциальных данных, как только это станет общедоступным. В ближайшем будущем Databricks Unity Catalog также позволит нативно использовать управление доступом на основе атрибутов, что упростит защиту конфиденциальных данных в масштабе.

Чтобы реализовать маскирование данных в модели dbt, убедитесь, что конфигурация материализации модели установлена на представление. Затем добавьте оператор case, используя функцию is_account_group_member для идентификации групп, которым разрешено просматривать значения в открытом виде. Затем используйте regex для маскирования данных для всех остальных пользователей. Например:

```sql
CASE
WHEN is_account_group_member('auditors') THEN email
ELSE regexp_extract(email, '^.*@(.*)$', 1)
END
```

Рекомендуется не предоставлять пользователям возможность читать таблицы и представления, на которые ссылается динамическое представление. Вместо этого назначьте ваши источники dbt динамическим представлениям, а не сырым данным, позволяя разработчикам безопасно выполнять полные сборки и команды свежести источников.

Использование одних и тех же источников для сред разработки и развертывания позволяет тестировать с теми же объемами и частотой, которые вы увидите в продакшене. Однако это может привести к тому, что запуски разработки будут занимать больше времени, чем необходимо. Чтобы решить эту проблему, рассмотрите возможность использования переменной Jinja target.name для [ограничения данных при работе в среде разработки](/reference/dbt-jinja-functions/target#use-targetname-to-limit-data-in-dev).

## Сочетание dbt Docs и Unity Catalog

Хотя между dbt docs и Databricks Unity Catalog есть сходства, они в конечном итоге используются для разных целей и хорошо дополняют друг друга. Объединяя их сильные стороны, вы можете предоставить вашей организации надежную и удобную экосистему управления данными.

dbt docs — это сайт документации, сгенерированный из вашего проекта dbt, который предоставляет интерфейс для разработчиков и нетехнических заинтересованных сторон, чтобы понять происхождение данных и бизнес-логику, применяемую к трансформациям, без необходимости полного доступа к dbt Cloud или Databricks. Это дает вам дополнительные возможности по организации и поиску ваших данных. Вы можете автоматически [создавать и просматривать ваши dbt docs с помощью dbt Cloud](/docs/collaborate/build-and-view-your-docs), чтобы поддерживать документацию в актуальном состоянии.

Unity Catalog — это единое решение для управления вашими lakehouse. Оно предоставляет исследователь данных, который можно использовать для обнаружения наборов данных, которые не были определены в dbt. Исследователь данных также фиксирует [происхождение на уровне столбцов](https://docs.databricks.com/data-governance/unity-catalog/data-lineage.html#capture-and-explore-lineage), когда вам нужно отследить происхождение конкретного столбца.

Чтобы получить максимальную отдачу от обоих инструментов, вы можете использовать [persist docs config](/reference/resource-configs/persist_docs) для передачи описаний таблиц и столбцов, написанных в dbt, в Unity Catalog, делая информацию легко доступной для пользователей обоих инструментов. Сохранение описаний в dbt гарантирует, что они находятся под контролем версий и могут быть воспроизведены после удаления таблицы.

### Связанные документы

- [Курс по продвинутому развертыванию](https://learn.getdbt.com/courses/advanced-deployment), если вы хотите углубиться в эти темы
- [Автомасштабирование CI: Интеллектуальный Slim CI](https://docs.getdbt.com/blog/intelligent-slim-ci)
- [Запуск задачи dbt Cloud в вашем автоматизированном рабочем процессе с помощью Python](https://discourse.getdbt.com/t/triggering-a-dbt-cloud-job-in-your-automated-workflow-with-python/2573)
- [Руководство по быстрому старту Databricks + dbt Cloud](/guides/databricks)
- Обратитесь к вашей команде аккаунта Databricks, чтобы получить доступ к функциям предварительного просмотра на Databricks.

</div>