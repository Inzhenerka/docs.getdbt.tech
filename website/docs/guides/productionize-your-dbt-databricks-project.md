---
title: Перевод вашего проекта dbt Databricks в продакшн
id: productionize-your-dbt-databricks-project
description: "Узнайте, как доставлять модели конечным пользователям и использовать лучшие практики для поддержания данных в продакшене." 
displayText: Перевод вашего проекта dbt Databricks в продакшн
hoverSnippet: Узнайте, как перевести ваш проект dbt Databricks в продакшн.
icon: 'databricks'
hide_table_of_contents: true
tags: ['Databricks','dbt Core','dbt platform']
level: 'Intermediate'
---

<div style={{maxWidth: '900px'}}>

## Введение {#introduction}

Добро пожаловать в третью часть нашей подробной серии, посвящённой оптимизации и развертыванию ваших конвейеров данных с использованием Databricks и <Constant name="cloud" />. В этом руководстве мы сосредоточимся на том, как доставлять эти модели конечным пользователям, одновременно внедряя лучшие практики, которые помогут обеспечить надёжность и своевременность данных в продакшене.

### Предварительные требования {#prerequisites}

Если у вас нет каких-либо из следующих требований, обратитесь к инструкциям в [Настройка вашего проекта dbt с Databricks](/guides/set-up-your-databricks-dbt-project) для помощи в их выполнении:

- Вы [настроили свой dbt‑проект с Databricks](/guides/set-up-your-databricks-dbt-project).
- Вы [оптимизировали свои dbt‑модели для максимальной производительности](/guides/optimize-dbt-models-on-databricks).
- Вы создали два каталога в Databricks: *dev* и *prod*.
- Вы создали Databricks Service Principal для запуска production‑джобов.
- У вас есть как минимум одно [deployment environment](/docs/deploy/deploy-environments) в <Constant name="cloud" />.

Чтобы начать, давайте вернемся к среде развертывания, созданной для ваших данных в продакшене.

### Среды развертывания {#deployment-environments}

В программной инженерии среды играют ключевую роль, позволяя инженерам разрабатывать и тестировать код, не влияя на конечных пользователей их программного обеспечения. Аналогично, вы можете проектировать [data lakehouses](https://www.databricks.com/product/data-lakehouse) с отдельными средами. _Продакшн_ среда включает в себя отношения (схемы, таблицы и представления), которые запрашивают или используют конечные пользователи, обычно в BI инструменте или модели ML.

В <Constant name="cloud" /> [окружения](/docs/dbt-cloud-environments) бывают двух типов:

- **Deployment** &mdash; определяет настройки, используемые для выполнения заданий (jobs), созданных в рамках этого окружения.
- **Development** &mdash; определяет настройки, используемые в <Constant name="cloud_ide" /> для конкретного проекта <Constant name="cloud" />.

Каждый проект <Constant name="cloud" /> может иметь несколько deployment-окружений, но только одно development-окружение на пользователя.

## Создание и планирование продакшн задачи {#create-and-schedule-a-production-job}

С вашей настроенной средой развертывания пришло время создать продакшн задачу для выполнения в вашей *prod* среде.

Чтобы развернуть наши рабочие процессы трансформации данных, мы будем использовать [встроенный планировщик заданий <Constant name="cloud" />](/docs/deploy/deploy-jobs). Планировщик заданий разработан специально для упрощения развертывания и выполнения проектов dbt, обеспечивая удобное создание, мониторинг и изменение ваших пайплайнов данных.

Использование планировщика заданий <Constant name="cloud" /> позволяет командам данных владеть всем процессом трансформации целиком. Вам не нужно изучать и поддерживать дополнительные инструменты оркестрации или зависеть от другой команды для планирования выполнения кода, написанного вашей командой. Такое сквозное владение упрощает процесс развертывания и ускоряет доставку новых продуктов данных.

Давайте [создадим задание](/docs/deploy/deploy-jobs#create-and-schedule-jobs) в <Constant name="cloud" />, которое будет выполнять трансформацию данных в нашем каталоге Databricks *prod*.

1. Создайте новую задачу, нажав **Deploy** в заголовке, затем **Jobs** и **Create job**.
2. **Назовите** задачу “Ежедневное обновление”.
3. Установите **Environment** на вашу *production* среду.
    - Это позволит задаче наследовать каталог, схему, учетные данные и переменные среды, определенные в [Настройка вашего проекта dbt с Databricks](/guides/set-up-your-databricks-dbt-project).
4. В разделе **Execution Settings**
    - Установите флажок **Generate docs on run**, чтобы настроить задачу на автоматическую генерацию документации проекта каждый раз, когда эта задача выполняется. Это обеспечит актуальность вашей документации по мере добавления и изменения моделей.
    - Выберите флажок **Run on source freshness**, чтобы настроить dbt [source freshness](/docs/deploy/source-freshness) как первый шаг этой задачи. Ваши источники должны быть настроены на [снимок информации о свежести](/docs/build/sources#source-data-freshness), чтобы это давало значимые инсайты.

    Добавьте следующие три **Commands:**
    - `dbt source freshness`
        - Это проверит, не устарели ли какие-либо источники. Мы не хотим пересчитывать модели с данными, которые не изменились с момента нашего последнего запуска.
    - `dbt test --models source:*`
        - Это проверит качество данных наших исходных данных, например, убедится, что поля ID уникальны и не пусты. Мы не хотим, чтобы плохие данные попадали в продакшн модели.
    - `dbt build --exclude source:* --fail-fast`
- `dbt build` более эффективен, чем запуск отдельных команд `dbt run` и `dbt test`, потому что он сначала выполняет модель, а затем сразу тестирует её, прежде чем переходить к следующей.
- Мы исключаем source-данные, потому что уже протестировали их на шаге 2.
- Флаг `fail-fast` заставляет dbt немедленно завершить работу, если хотя бы один ресурс не смог собраться. Если в момент сбоя первой модели другие модели ещё выполняются, dbt завершит подключения для этих всё ещё выполняющихся моделей.

5. В разделе **Triggers** с помощью переключателя настройте ваш job на [запуск по расписанию](/docs/deploy/deploy-jobs#schedule-days). Вы можете указать конкретные дни и время либо создать собственное cron-расписание.  
    - Если вы хотите, чтобы ваш job в <Constant name="cloud" /> запускался другим оркестратором, например Databricks Workflows, см. раздел [Advanced Considerations](#advanced-considerations) ниже.

Это всего лишь один пример списка команд "все или ничего", предназначенного для минимизации потерь вычислительных ресурсов. [Список команд задач](/docs/deploy/job-commands) и [селекторы](/reference/node-selection/syntax) предоставляют большую гибкость в том, как будет выполняться ваш DAG. Вы можете захотеть спроектировать свой так, чтобы продолжать выполнение определенных моделей, если другие не удаются. Вы можете захотеть настроить несколько задач для обновления моделей с разной частотой. См. наш [Job Creation Best Practices discourse](https://discourse.getdbt.com/t/job-creation-best-practices-in-dbt-cloud-feat-my-moms-lasagna/2980) для получения дополнительных предложений по проектированию задач.

После того как ваша задача настроена и успешно выполняется, настройте ваши **[артефакты проекта](/docs/deploy/artifacts)**, чтобы эта задача информировала ваш сайт документации продакшн и панель данных источников, к которым можно получить доступ из пользовательского интерфейса.

Это будет наша основная продакшн задача для обновления данных, которые будут использоваться конечными пользователями. Еще одна задача, которую все должны включить в свой проект dbt, это задача непрерывной интеграции.

## Добавление задачи CI {#add-a-ci-job}

CI/CD, или непрерывная интеграция и непрерывное развертывание/доставка, стало стандартной практикой в разработке программного обеспечения для быстрого выпуска новых функций и исправлений ошибок при сохранении высокого качества и стабильности. <Constant name="cloud" /> позволяет применять эти практики к вашим преобразованиям данных.

Ниже описаны шаги по созданию CI‑теста для вашего dbt‑проекта. CD в <Constant name="cloud" /> не требует дополнительных шагов, так как ваши jobs автоматически подхватывают последние изменения из ветки, назначенной среде, в которой выполняется job. В зависимости от вашей стратегии развертывания вы можете добавить дополнительные шаги. Если вы хотите глубже разобраться в вариантах CD, ознакомьтесь с [этой статьей в блоге о внедрении CI/CD с помощью <Constant name="cloud" />](https://www.getdbt.com/blog/adopting-ci-cd-with-dbt-cloud/).

dbt позволяет писать [data tests](/docs/build/data-tests) для вашего data pipeline, которые можно запускать на каждом этапе процесса, чтобы обеспечить стабильность и корректность преобразований данных. Основные места, где вы будете использовать dbt‑тесты, следующие:

1. **Ежедневные запуски:** Регулярное выполнение тестов на вашем конвейере данных помогает выявлять проблемы, вызванные плохими исходными данными, обеспечивая качество данных, которые достигают ваших пользователей.
2. **Разработка**: Выполнение тестов во время разработки гарантирует, что изменения в вашем коде не нарушают существующие предположения, позволяя разработчикам быстрее итеративно работать, выявляя проблемы сразу после написания кода.
3. **Проверки CI**: Автоматизированные задачи CI выполняют и тестируют ваш конвейер от начала до конца, когда создается запрос на слияние, обеспечивая уверенность разработчикам, рецензентам кода и конечным пользователям, что предлагаемые изменения надежны и не вызовут сбоев или проблем с качеством данных.

Ваша задача CI будет гарантировать, что модели правильно строятся и проходят любые примененные к ним тесты. Мы рекомендуем создать отдельную *тестовую* среду и иметь выделенный сервисный принципал. Это обеспечит, что временные схемы, создаваемые во время тестов CI, находятся в своем собственном каталоге и не могут случайно раскрыть данные другим пользователям. Повторите шаги в [Настройка вашего проекта dbt с Databricks](/guides/set-up-your-databricks-dbt-project) для создания вашей *prod* среды, чтобы создать *тестовую* среду. После настройки у вас должно быть:

- Каталог с именем *test*
- Сервисный принципал с именем *dbt_test_sp*
- Новая среда <Constant name="cloud" /> с именем *test*, которая по умолчанию использует каталог *test* и применяет токен *dbt_test_sp* в учетных данных деплоя

Мы рекомендуем настроить CI‑задачу в <Constant name="cloud" />. Это позволит сократить время выполнения задач за счет запуска и тестирования только изменённых моделей, а также снизить вычислительные затраты в lakehouse. Подробные инструкции по созданию CI‑задачи см. в разделе [Set up CI jobs](/docs/deploy/ci-jobs).

С тестами dbt и SlimCI вы можете быть уверены, что ваши данные в продакшене будут своевременными и точными, даже при высокой скорости доставки.

## Мониторинг ваших задач {#monitor-your-jobs}

Внимательное отслеживание заданий в <Constant name="cloud" /> имеет решающее значение для поддержания надёжного и эффективного конвейера данных. Мониторинг производительности заданий и быстрое выявление потенциальных проблем позволяют убедиться, что ваши преобразования данных выполняются корректно. <Constant name="cloud" /> предоставляет три точки входа для мониторинга состояния проекта: историю запусков, мониторинг деплоев и статусные плитки.

Дашборд [run history](/docs/deploy/run-visibility#run-history) в <Constant name="cloud" /> предоставляет детальный обзор всех запусков заданий вашего проекта, а также различные фильтры, которые помогают сосредоточиться на нужных аспектах. Это отличный инструмент для разработчиков, которые хотят проверить последние запуски, убедиться в корректности ночных выполнений или отследить прогресс текущих заданий. Чтобы открыть его, выберите **Run History** в меню **Deploy**.

Мониторинг деплоев в <Constant name="cloud" /> предоставляет более высокоуровневое представление истории запусков и позволяет оценить состояние вашего конвейера данных за более длительный период времени. Эта функция включает информацию о длительности запусков и показателях успешности, что помогает выявлять тенденции в производительности заданий, например, рост времени выполнения или увеличение числа сбоев. Мониторинг деплоев также подсвечивает выполняющиеся задания, задания в очереди и недавние ошибки. Чтобы открыть мониторинг деплоев, нажмите на логотип dbt в левом верхнем углу интерфейса <Constant name="cloud" />.

<Lightbox src="/img/guides/databricks-guides/deployment_monitor_dbx.png" width="85%" title="Монитор развертывания показывает статус задач с течением времени в разных средах" />

Добавляя **[data health tiles](/docs/explore/data-tile)** в ваши BI-дашборды, вы даёте стейкхолдерам наглядное представление о состоянии вашего data pipeline, не заставляя их покидать привычный интерфейс. Data tiles повышают доверие к данным и помогают избежать лишних вопросов или необходимости переключаться между инструментами. Чтобы реализовать status tiles на дашбордах, у вас должна быть настроена документация dbt с определёнными **[exposures](/docs/build/exposures)**.

## Настройка уведомлений {#set-up-notifications}

Настройка [уведомлений](/docs/deploy/job-notifications) в <Constant name="cloud" /> позволяет получать оповещения по электронной почте или в Slack‑канал каждый раз, когда выполнение задания завершается. Это гарантирует, что соответствующие команды будут уведомлены и смогут оперативно отреагировать в случае сбоя или отмены задания. Чтобы настроить уведомления:

1. Перейдите в настройки проекта <Constant name="cloud" />.
2. Выберите вкладку **Notifications**.
3. Выберите нужный тип уведомлений (Email или Slack) и настройте соответствующие параметры.

Если вам требуются уведомления через другие каналы, помимо электронной почты или Slack, вы можете использовать функцию исходящих [webhooks](/docs/deploy/webhooks) в <Constant name="cloud" /> для передачи событий заданий в другие инструменты. Webhooks позволяют интегрировать <Constant name="cloud" /> с широким спектром SaaS‑приложений, расширяя автоматизацию вашего пайплайна на другие системы.

## Устранение неполадок {#troubleshooting}

Когда в вашем продакшн конвейере происходит сбой, важно знать, как эффективно устранять проблемы, чтобы минимизировать время простоя и поддерживать высокий уровень доверия со стороны ваших заинтересованных сторон.

Пять ключевых шагов для устранения проблем с <Constant name="cloud" />:

1. Прочитайте сообщение об ошибке: сообщения об ошибках dbt обычно указывают тип ошибки и файл, в котором она произошла.
2. Проверьте проблемный файл и поищите очевидное решение.
3. Изолируйте проблему, запуская по одной модели в <Constant name="cloud_ide" /> или откатывая код, который вызвал ошибку.
4. Проверьте наличие проблем в скомпилированных файлах и логах.

Обратитесь к [документации по отладке ошибок](/guides/debug-errors) для получения полного списка типов ошибок и методов диагностики.

Чтобы устранить проблемы с заданием <Constant name="cloud" />, перейдите на вкладку **Deploy > Run History** в вашем проекте <Constant name="cloud" /> и выберите неудачный запуск. Затем разверните шаги выполнения, чтобы просмотреть [console и debug логи](/docs/deploy/run-visibility#access-logs) и изучить подробные сообщения журнала. Чтобы получить дополнительную информацию, откройте вкладку **Artifacts** и скачайте скомпилированные файлы, связанные с этим запуском.

Если ваши задачи занимают больше времени, чем ожидалось, используйте панель [времени выполнения модели](/docs/deploy/run-visibility#model-timing) для выявления узких мест в вашем конвейере. Анализ времени, затраченного на выполнение каждой модели, помогает вам определить самые медленные компоненты и оптимизировать их для повышения производительности. История запросов Databricks [Query History](https://docs.databricks.com/sql/admin/query-history.html) позволяет вам изучать детальные сведения, такие как время, затраченное на каждую задачу, возвращенные строки, производительность ввода-вывода и план выполнения.

Для получения дополнительной информации о настройке производительности ознакомьтесь с нашим руководством по [Как оптимизировать и устранять неполадки моделей dbt на Databricks](/guides/optimize-dbt-models-on-databricks).

## Продвинутые соображения {#advanced-considerations}

По мере того как вы будете набираться опыта работы с <Constant name="cloud" /> и Databricks, вам может понадобиться изучить более продвинутые техники, которые помогут дополнительно улучшить ваш data pipeline и оптимизировать управление преобразованиями данных. Темы в этом разделе не являются обязательными, но они помогут укрепить production‑окружение, повысив уровень безопасности, эффективности и доступности.

### Обновление ваших данных с помощью Databricks Workflows {#refreshing-your-data-with-databricks-workflows}

Планировщик заданий <Constant name="cloud" /> предлагает несколько способов запуска ваших заданий. Если преобразования dbt — это лишь один из шагов в более крупном оркестрационном workflow, используйте API <Constant name="cloud" />, чтобы запускать ваше задание из Databricks Workflows.

Это распространенный шаблон для аналитических случаев использования, которые хотят минимизировать задержку между загрузкой бронзовых данных в lakehouse с помощью ноутбука, преобразованием этих данных в золотые таблицы с помощью dbt и обновлением панели. Это также полезно для команд по науке о данных, которые используют dbt для извлечения признаков перед использованием обновленного хранилища признаков для обучения и регистрации моделей машинного обучения с MLflow.

API обеспечивает интеграцию между вашими заданиями <Constant name="cloud" /> и workflow Databricks, гарантируя, что ваши преобразования данных эффективно управляются в более широком контексте конвейера обработки данных.

Добавление заданий <Constant name="cloud" /> в Databricks Workflows позволяет выстраивать цепочки внешних задач, при этом продолжая использовать следующие преимущества <Constant name="cloud" />:

- **Контекст UI**: UI <Constant name="cloud" /> позволяет определять задания в контексте ваших окружений <Constant name="cloud" />, что упрощает создание и управление соответствующими конфигурациями.
- **Логи и история запусков**: Доступ к логам и истории запусков становится более удобным при использовании <Constant name="cloud" />.
- **Возможности мониторинга и уведомлений**: <Constant name="cloud" /> оснащён функциями мониторинга и уведомлений, подобными описанным выше, которые помогают оставаться в курсе статуса и производительности ваших заданий.

Чтобы запускать задания <Constant name="cloud" /> из Databricks, следуйте инструкциям в нашем руководстве [Databricks Workflows to run <Constant name="cloud" /> jobs guide](/guides/how-to-use-databricks-workflows-to-run-dbt-cloud-jobs).

## Маскирование данных {#data-masking}

В нашем руководстве [Best Practices for dbt and Unity Catalog](/best-practices/dbt-unity-catalog-best-practices) рекомендуется использовать отдельные каталоги *dev* и *prod* для сред разработки и развертывания, при этом Unity Catalog и <Constant name="cloud" /> берут на себя управление конфигурациями и правами доступа для изоляции окружений. Обеспечение безопасности при сохранении эффективности в средах разработки и развертывания имеет решающее значение. Для защиты чувствительных данных, таких как персональные данные (PII), могут потребоваться дополнительные меры безопасности.

Databricks использует [Динамические представления](https://docs.databricks.com/data-governance/unity-catalog/create-views.html#create-a-dynamic-view) для включения маскирования данных на основе членства в группе. Поскольку представления в Unity Catalog используют Spark SQL, вы можете реализовать продвинутое маскирование данных, используя более сложные SQL выражения и регулярные выражения. Вы также можете применять более детализированные средства управления доступом, такие как фильтры строк в предварительном просмотре и маски столбцов в предварительном просмотре на таблицах в Databricks Unity Catalog, что будет рекомендованным подходом для защиты конфиденциальных данных, как только это станет общедоступным. В ближайшем будущем Databricks Unity Catalog также позволит нативно использовать управление доступом на основе атрибутов, что упростит защиту конфиденциальных данных в масштабе.

Чтобы реализовать маскирование данных в модели dbt, убедитесь, что конфигурация материализации модели установлена на представление. Затем добавьте оператор case, используя функцию is_account_group_member для идентификации групп, которым разрешено просматривать значения в открытом виде. Затем используйте regex для маскирования данных для всех остальных пользователей. Например:

```sql
CASE
WHEN is_account_group_member('auditors') THEN email
ELSE regexp_extract(email, '^.*@(.*)$', 1)
END
```

Рекомендуется не предоставлять пользователям возможность читать таблицы и представления, на которые ссылается динамическое представление. Вместо этого назначьте ваши источники dbt динамическим представлениям, а не сырым данным, позволяя разработчикам безопасно выполнять полные сборки и команды свежести источников.

Использование одних и тех же источников для сред разработки и развертывания позволяет тестировать с теми же объемами и частотой, которые вы увидите в продакшене. Однако это может привести к тому, что запуски разработки будут занимать больше времени, чем необходимо. Чтобы решить эту проблему, рассмотрите возможность использования переменной Jinja target.name для [ограничения данных при работе в среде разработки](/reference/dbt-jinja-functions/target#use-targetname-to-limit-data-in-dev).

## Сочетание dbt Docs и Unity Catalog {#pairing-dbt-docs-and-unity-catalog}

Хотя между dbt docs и Databricks Unity Catalog есть сходства, они в конечном итоге используются для разных целей и хорошо дополняют друг друга. Объединяя их сильные стороны, вы можете предоставить вашей организации надежную и удобную экосистему управления данными.

dbt docs — это сайт с документацией, который генерируется на основе вашего dbt‑проекта и предоставляет интерфейс как для разработчиков, так и для нетехнических стейкхолдеров. Он позволяет понять линейность данных и бизнес‑логику, применяемую в трансформациях, без необходимости полного доступа к <Constant name="cloud" /> или Databricks. Документация предоставляет дополнительные возможности для организации и поиска данных. Вы можете автоматически [собирать и просматривать dbt docs с помощью <Constant name="cloud" />](/docs/explore/build-and-view-your-docs), чтобы документация всегда оставалась актуальной.

Unity Catalog — это единое решение для управления вашими lakehouse. Оно предоставляет исследователь данных, который можно использовать для обнаружения наборов данных, которые не были определены в dbt. Исследователь данных также фиксирует [происхождение на уровне столбцов](https://docs.databricks.com/data-governance/unity-catalog/data-lineage.html#capture-and-explore-lineage), когда вам нужно отследить происхождение конкретного столбца.

Чтобы получить максимальную отдачу от обоих инструментов, вы можете использовать [persist docs config](/reference/resource-configs/persist_docs) для передачи описаний таблиц и столбцов, написанных в dbt, в Unity Catalog, делая информацию легко доступной для пользователей обоих инструментов. Сохранение описаний в dbt гарантирует, что они находятся под контролем версий и могут быть воспроизведены после удаления таблицы.

### Связанные документы {#related-docs}

- [Курс Advanced Deployment](https://learn.getdbt.com/courses/advanced-deployment), если вы хотите более глубоко разобраться в этих темах
- [Autoscaling CI: интеллектуальный Slim CI](/docs/deploy/continuous-integration)
- [Запуск задания <Constant name="cloud" /> в вашем автоматизированном рабочем процессе с помощью Python](https://discourse.getdbt.com/t/triggering-a-dbt-cloud-job-in-your-automated-workflow-with-python/2573)
- [Краткое руководство Databricks + <Constant name="cloud" />](/guides/databricks)
- Обратитесь к вашей команде аккаунта Databricks, чтобы получить доступ к превью‑функциям Databricks.

</div>