---
title: Продуктивное использование вашего проекта dbt на Databricks
id: productionize-your-dbt-databricks-project
description: "Узнайте, как доставлять модели конечным пользователям и использовать лучшие практики для поддержания производственных данных." 
displayText: Продуктивное использование вашего проекта dbt на Databricks
hoverSnippet: Узнайте, как продуктивно использовать ваш проект dbt на Databricks.
# time_to_complete: '30 минут' временно отключено до тестирования
icon: 'databricks'
hide_table_of_contents: true
tags: ['Databricks','dbt Core','dbt Cloud']
level: 'Средний'
recently_updated: true
---

<div style={{maxWidth: '900px'}}>

## Введение

Добро пожаловать в третью часть нашей обширной серии по оптимизации и развертыванию ваших данных с использованием Databricks и dbt Cloud. В этом руководстве мы углубимся в доставку этих моделей конечным пользователям, одновременно внедряя лучшие практики, чтобы гарантировать, что ваши производственные данные остаются надежными и актуальными.

### Предварительные требования

Если у вас нет каких-либо из следующих требований, обратитесь к инструкциям в [Настройка вашего проекта dbt с Databricks](/guides/set-up-your-databricks-dbt-project) для помощи в выполнении этих требований:

- Вы настроили [ваш проект dbt с Databricks](/guides/set-up-your-databricks-dbt-project).
- Вы [оптимизировали ваши модели dbt для максимальной производительности](/guides/optimize-dbt-models-on-databricks).
- Вы создали два каталога в Databricks: *dev* и *prod*.
- Вы создали сервисный принципал Databricks для выполнения ваших производственных заданий.
- У вас есть как минимум одна [среда развертывания](/docs/deploy/deploy-environments) в dbt Cloud.

Чтобы начать, давайте вспомним о среде развертывания, созданной для ваших производственных данных.

### Среды развертывания

В программной инженерии среды играют ключевую роль, позволяя инженерам разрабатывать и тестировать код, не влияя на конечных пользователей их программного обеспечения. Аналогично, вы можете проектировать [data lakehouses](https://www.databricks.com/product/data-lakehouse) с отдельными средами. Среда _производства_ включает отношения (схемы, таблицы и представления), которые запрашивают или используют конечные пользователи, обычно в инструменте BI или модели ML.

В dbt Cloud [среды](/docs/dbt-cloud-environments) бывают двух типов:

- Развертывание &mdash; Определяет настройки, используемые для выполнения заданий, созданных в этой среде.
- Разработка &mdash; Определяет настройки, используемые в IDE dbt Cloud для конкретного проекта dbt Cloud. 

Каждый проект dbt Cloud может иметь несколько сред развертывания, но только одну среду разработки на пользователя.

## Создание и планирование производственного задания

С вашей средой развертывания настроенной, пришло время создать производственное задание для выполнения в вашей *prod* среде.

Для развертывания наших рабочих процессов преобразования данных мы будем использовать [встроенный планировщик заданий dbt Cloud](/docs/deploy/deploy-jobs). Планировщик заданий специально разработан для упрощения развертывания и выполнения вашего проекта dbt, обеспечивая легкость создания, мониторинга и эффективного изменения ваших потоков данных.

Использование планировщика заданий dbt Cloud позволяет командам данных владеть всем рабочим процессом преобразования. Вам не нужно изучать и поддерживать дополнительные инструменты для оркестрации или полагаться на другую команду для планирования кода, написанного вашей командой. Это полное владение упрощает процесс развертывания и ускоряет доставку новых продуктов данных.

Давайте [создадим задание](/docs/deploy/deploy-jobs#create-and-schedule-jobs) в dbt Cloud, которое будет преобразовывать данные в нашем каталоге Databricks *prod*.

1. Создайте новое задание, нажав **Deploy** в заголовке, затем **Jobs** и **Create job**.
2. **Назовите** задание "Ежедневное обновление".
3. Установите **Среду** на вашу *производственную* среду.
    - Это позволит заданию унаследовать каталог, схему, учетные данные и переменные окружения, определенные в [Настройка вашего проекта dbt с Databricks](/guides/set-up-your-databricks-dbt-project).
4. В разделе **Настройки выполнения**
    - Установите флажок **Генерировать документацию при выполнении**, чтобы настроить задание на автоматическую генерацию документации проекта каждый раз, когда это задание выполняется. Это обеспечит актуальность вашей документации по мере добавления и изменения моделей.
    - Выберите флажок **Запуск на свежести источника**, чтобы настроить dbt [свежесть источника](/docs/deploy/source-freshness) в качестве первого шага этого задания. Ваши источники должны быть настроены для [снимка информации о свежести](/docs/build/sources#source-data-freshness), чтобы это дало значимые результаты.
    
    Добавьте следующие три **Команды:**
    - `dbt source freshness`
        - Это проверит, есть ли устаревшие источники. Мы не хотим пересчитывать модели с данными, которые не изменились с момента нашего последнего выполнения.
    - `dbt test --models source:*`
        - Это протестирует качество данных наших исходных данных, например, проверяя, чтобы поля ID были уникальными и не содержали null. Мы не хотим, чтобы плохие данные попали в производственные модели.
    - `dbt build --exclude source:* --fail-fast`
        - dbt build более эффективен, чем выполнение отдельных команд для dbt run и dbt test, так как он сначала выполнит, а затем протестирует каждую модель перед продолжением.
        - Мы исключаем исходные данные, потому что уже протестировали их на шаге 2.
        - Флаг fail-fast заставит dbt немедленно завершить работу, если один ресурс не удалось построить. Если другие модели находятся в процессе выполнения, когда первая модель терпит неудачу, dbt завершит соединения для этих все еще работающих моделей.
5. В разделе **Триггеры** используйте переключатель, чтобы настроить ваше задание на [выполнение по расписанию](/docs/deploy/deploy-jobs#schedule-days). Вы можете ввести конкретные дни и время или создать пользовательское расписание cron. 
    - Если вы хотите, чтобы ваше задание dbt Cloud планировалось другим оркестратором, таким как Databricks Workflows, смотрите раздел [Расширенные соображения](#advanced-considerations) ниже.

Это всего лишь один пример списка команд "все или ничего", разработанного для минимизации потерь вычислительных ресурсов. [Список команд задания](/docs/deploy/job-commands) и [селекторы](/reference/node-selection/syntax) предоставляют много гибкости в том, как будет выполняться ваш DAG. Вы можете захотеть спроектировать его так, чтобы продолжать выполнение определенных моделей, если другие потерпят неудачу. Вы можете захотеть настроить несколько заданий для обновления моделей с разной частотой. Ознакомьтесь с нашими [Лучшие практики создания заданий](https://discourse.getdbt.com/t/job-creation-best-practices-in-dbt-cloud-feat-my-moms-lasagna/2980) для получения дополнительных предложений по дизайну заданий.

После настройки вашего задания и его успешного выполнения, настройте ваши **[артефакты проекта](/docs/deploy/artifacts)**, чтобы это задание информировало ваш сайт документации и панель инструментов источников данных, доступные из интерфейса.

Это будет наше основное производственное задание для обновления данных, которые будут использоваться конечными пользователями. Еще одно задание, которое все должны включить в свой проект dbt, это задание непрерывной интеграции.

## Добавление CI задания

CI/CD, или Непрерывная интеграция и Непрерывное развертывание/доставка, стало стандартной практикой в разработке программного обеспечения для быстрого предоставления новых функций и исправлений ошибок при поддержании высокого качества и стабильности. dbt Cloud позволяет вам применять эти практики к вашим преобразованиям данных.

Ниже приведены шаги по созданию CI теста для вашего проекта dbt. CD в dbt Cloud не требует дополнительных шагов, так как ваши задания автоматически будут подхватывать последние изменения из ветки, назначенной для среды, в которой выполняется ваше задание. Вы можете выбрать добавление шагов в зависимости от вашей стратегии развертывания. Если вы хотите углубиться в варианты CD, ознакомьтесь с [этим блогом о внедрении CI/CD с dbt Cloud](https://www.getdbt.com/blog/adopting-ci-cd-with-dbt-cloud/).

dbt позволяет вам писать [тесты](/docs/build/data-tests) для вашего потока данных, которые могут выполняться на каждом этапе процесса, чтобы гарантировать стабильность и корректность ваших преобразований данных. Основные места, где вы будете использовать ваши тесты dbt, это:

1. **Ежедневные запуски:** Регулярное выполнение тестов на вашем потоке данных помогает выявлять проблемы, вызванные плохими исходными данными, обеспечивая качество данных, которые достигают ваших пользователей.
2. **Разработка**: Выполнение тестов во время разработки гарантирует, что изменения в вашем коде не нарушают существующие предположения, позволяя разработчикам быстрее итеративно работать, выявляя проблемы сразу после написания кода.
3. **CI проверки**: Автоматизированные CI задания выполняются и тестируют ваш поток данных от начала до конца, когда создается запрос на слияние, предоставляя уверенность разработчикам, рецензентам кода и конечным пользователям в том, что предложенные изменения надежны и не вызовут сбоев или проблем с качеством данных.

Ваше CI задание обеспечит правильное построение моделей и прохождение любых примененных к ним тестов. Мы рекомендуем создать отдельную *тестовую* среду и иметь выделенный сервисный принципал. Это обеспечит, что временные схемы, созданные во время CI тестов, находятся в своем собственном каталоге и не могут случайно раскрыть данные другим пользователям. Повторите шаги в [Настройка вашего проекта dbt с Databricks](/guides/set-up-your-databricks-dbt-project), чтобы создать вашу *prod* среду для создания *тестовой* среды. После настройки у вас должно быть:

- Каталог под названием *test*
- Сервисный принципал под названием *dbt_test_sp*
- Новая среда dbt Cloud под названием *test*, которая по умолчанию использует каталог *test* и токен *dbt_test_sp* в учетных данных развертывания.

Мы рекомендуем настроить CI задание в dbt Cloud. Это уменьшит время выполнения задания, выполняя и тестируя только измененные модели, что также снизит затраты на вычисления в lakehouse. Для создания CI задания обратитесь к [Настройка CI заданий](/docs/deploy/ci-jobs) для получения подробностей.

С тестами dbt и SlimCI вы можете быть уверены, что ваши производственные данные будут актуальными и точными, даже при высокой скорости доставки.

## Мониторинг ваших заданий

Тщательный мониторинг ваших заданий dbt Cloud имеет решающее значение для поддержания надежного и эффективного потока данных. Путем мониторинга производительности заданий и быстрого выявления потенциальных проблем вы можете гарантировать, что ваши преобразования данных выполняются гладко. dbt Cloud предоставляет три точки доступа для мониторинга состояния вашего проекта: история запусков, монитор развертывания и статусные плитки.

Дашборд [истории запусков](/docs/deploy/run-visibility#run-history) в dbt Cloud предоставляет детальный обзор всех запусков заданий вашего проекта, предлагая различные фильтры, чтобы помочь вам сосредоточиться на конкретных аспектах. Это отличный инструмент для разработчиков, которые хотят проверить недавние запуски, подтвердить результаты за ночь или отслеживать прогресс выполняющихся заданий. Чтобы получить доступ к нему, выберите **Run History** в меню **Deploy**.

Монитор развертывания в dbt Cloud предлагает более высокий уровень обзора вашей истории запусков, позволяя вам оценить состояние вашего потока данных на протяжении длительного времени. Эта функция включает информацию о продолжительности запусков и коэффициентах успеха, позволяя вам выявлять тенденции в производительности заданий, такие как увеличение времени выполнения или более частые сбои. Монитор развертывания также выделяет задания в процессе выполнения, в очереди и недавние сбои. Чтобы получить доступ к монитору развертывания, нажмите на логотип dbt в верхнем левом углу интерфейса dbt Cloud.

<Lightbox src="/img/guides/databricks-guides/deployment_monitor_dbx.png" width="85%" title="Монитор развертывания показывает статус заданий с течением времени по средам" />

Добавив [плитки состояния данных](/docs/collaborate/data-tile) на ваши BI дашборды, вы можете предоставить заинтересованным сторонам видимость состояния вашего потока данных, не покидая их предпочтительный интерфейс. Плитки данных внушают уверенность в ваших данных и помогают предотвратить ненужные запросы или переключение контекста. Чтобы реализовать статусные плитки на дашборде, вам нужно иметь документацию dbt с определенными [exposures](/docs/build/exposures).

## Настройка уведомлений

Настройка [уведомлений](/docs/deploy/job-notifications) в dbt Cloud позволяет вам получать оповещения по электронной почте или в канале Slack всякий раз, когда выполнение заканчивается. Это гарантирует, что соответствующие команды уведомлены и могут быстро предпринять действия, когда задания терпят неудачу или отменяются. Чтобы настроить уведомления:

1. Перейдите в настройки вашего проекта dbt Cloud.
2. Выберите вкладку **Notifications**.
3. Выберите желаемый тип уведомления (Email или Slack) и настройте соответствующие параметры.

Если вам нужны уведомления через другие средства, кроме электронной почты или Slack, вы можете использовать функцию исходящих [вебхуков](/docs/deploy/webhooks) dbt Cloud для передачи событий заданий в другие инструменты. Вебхуки позволяют интегрировать dbt Cloud с широким спектром SaaS приложений, расширяя автоматизацию вашего потока данных в другие системы.

## Устранение неполадок

Когда происходит сбой в вашем производственном потоке, важно знать, как эффективно устранять проблемы, чтобы минимизировать время простоя и поддерживать высокий уровень доверия со стороны заинтересованных сторон.

Пять ключевых шагов для устранения неполадок в dbt Cloud:

1. Прочитайте сообщение об ошибке: сообщения об ошибках dbt обычно указывают тип ошибки и файл, в котором возникла проблема.
2. Проверьте проблемный файл и ищите немедленное решение.
3. Изолируйте проблему, выполняя одну модель за раз в IDE или отменяя код, вызвавший проблему.
4. Проверьте наличие проблем в скомпилированных файлах и журналах.

Обратитесь к [документации по отладке ошибок](/guides/debug-errors) для получения полного списка типов ошибок и методов диагностики.

Чтобы устранить неполадки с заданием dbt Cloud, перейдите на вкладку "Deploy > Run History" в вашем проекте dbt Cloud и выберите неудавшийся запуск. Затем разверните шаги запуска, чтобы просмотреть [журналы консоли и отладки](/docs/deploy/run-visibility#access-logs) для анализа детализированных сообщений журнала. Чтобы получить дополнительную информацию, откройте вкладку Артефакты и загрузите скомпилированные файлы, связанные с запуском.

Если ваши задания занимают больше времени, чем ожидалось, используйте дашборд [времени выполнения моделей](/docs/deploy/run-visibility#model-timing), чтобы выявить узкие места в вашем потоке данных. Анализ времени выполнения каждой модели помогает вам определить самые медленные компоненты и оптимизировать их для повышения производительности. [История запросов Databricks](https://docs.databricks.com/sql/admin/query-history.html) позволяет вам просматривать детализированные сведения, такие как время, затраченное на каждую задачу, возвращенные строки, производительность ввода-вывода и план выполнения.

Для получения дополнительной информации о настройке производительности ознакомьтесь с нашим руководством о [Как оптимизировать и устранять неполадки моделей dbt на Databricks](/guides/optimize-dbt-models-on-databricks).

## Расширенные соображения

По мере того как вы становитесь более опытными в dbt Cloud и Databricks, вы можете захотеть изучить расширенные техники для дальнейшего улучшения вашего потока данных и улучшения управления вашими преобразованиями данных. Темы в этом разделе не являются обязательными, но помогут вам укрепить вашу производственную среду для большей безопасности, эффективности и доступности.

### Обновление ваших данных с помощью Databricks Workflows

Планировщик заданий dbt Cloud предлагает несколько способов запуска ваших заданий. Если ваши преобразования dbt являются лишь одним шагом более крупного рабочего процесса оркестрации, используйте API dbt Cloud для запуска вашего задания из Databricks Workflows.

Это распространенный шаблон для аналитических случаев, которые хотят минимизировать задержку между загрузкой бронзовых данных в lakehouse с помощью блокнота, преобразованием этих данных в золотые таблицы с помощью dbt и обновлением дашборда. Это также полезно для команд по анализу данных, которые используют dbt для извлечения признаков перед использованием обновленного хранилища признаков для обучения и регистрации моделей машинного обучения с помощью MLflow.

API обеспечивает интеграцию между вашими заданиями dbt Cloud и рабочим процессом Databricks, гарантируя, что ваши преобразования данных эффективно управляются в более широком контексте вашего потока обработки данных.

Вставка заданий dbt Cloud в Databricks Workflows позволяет вам связывать внешние задачи, при этом используя преимущества dbt Cloud:

- Контекст UI: Интерфейс dbt Cloud позволяет вам определить задание в контексте ваших сред dbt Cloud, что упрощает создание и управление соответствующими конфигурациями.
- Журналы и история запусков: Доступ к журналам и истории запусков становится более удобным при использовании dbt Cloud.
- Функции мониторинга и уведомлений: dbt Cloud оснащен функциями мониторинга и уведомлений, такими как описанные выше, которые могут помочь вам оставаться в курсе состояния и производительности ваших заданий.

Чтобы запустить ваше задание dbt Cloud из Databricks, следуйте инструкциям в нашем [руководстве по использованию Databricks Workflows для запуска заданий dbt Cloud](/guides/how-to-use-databricks-workflows-to-run-dbt-cloud-jobs).

## Маскировка данных

Наше руководство по [Лучшим практикам для dbt и Unity Catalog](/best-practices/dbt-unity-catalog-best-practices) рекомендует использовать отдельные каталоги *dev* и *prod* для сред разработки и развертывания, при этом Unity Catalog и dbt Cloud обрабатывают конфигурации и разрешения для изоляции среды. Обеспечение безопасности при поддержании эффективности в ваших средах разработки и развертывания имеет решающее значение. Дополнительные меры безопасности могут быть необходимы для защиты конфиденциальных данных, таких как личная информация (PII).

Databricks использует [Динамические представления](https://docs.databricks.com/data-governance/unity-catalog/create-views.html#create-a-dynamic-view) для включения маскировки данных на основе членства в группах. Поскольку представления в Unity Catalog используют Spark SQL, вы можете реализовать сложную маскировку данных, используя более сложные SQL выражения и регулярные выражения. Вы также можете применять детализированные контроль доступа, такие как фильтры строк в предварительном просмотре и маски столбцов в предварительном просмотре на таблицах в Databricks Unity Catalog, что будет рекомендованным подходом для защиты конфиденциальных данных, как только это станет общедоступным. Кроме того, в ближайшем будущем Databricks Unity Catalog также позволит нативно использовать контроль доступа на основе атрибутов, что упростит защиту конфиденциальных данных в масштабах.

Чтобы реализовать маскировку данных в модели dbt, убедитесь, что конфигурация материализации модели установлена на представление. Затем добавьте оператор case, используя функцию is_account_group_member для определения групп, которым разрешено видеть значения в открытом виде. Затем используйте регулярные выражения для маскировки данных для всех других пользователей. Например:

```sql
CASE
WHEN is_account_group_member('auditors') THEN email
ELSE regexp_extract(email, '^.*@(.*)$', 1)
END
```

Рекомендуется не предоставлять пользователям возможность читать таблицы и представления, на которые ссылается динамическое представление. Вместо этого назначьте ваши источники dbt на динамические представления, а не на необработанные данные, позволяя разработчикам безопасно выполнять полные сборки и команды свежести источников.

Использование одних и тех же источников для сред разработки и развертывания позволяет тестировать с теми же объемами и частотой, которые вы увидите в производстве. Однако это может привести к тому, что запуски разработки займут больше времени, чем необходимо. Чтобы решить эту проблему, рассмотрите возможность использования переменной Jinja target.name для [ограничения данных при работе в среде разработки](/reference/dbt-jinja-functions/target#use-targetname-to-limit-data-in-dev).

## Сочетание документации dbt и Unity Catalog

Хотя между документацией dbt и Databricks Unity Catalog есть сходства, они в конечном итоге используются для разных целей и хорошо дополняют друг друга. Объединив их сильные стороны, вы можете предоставить вашей организации надежную и удобную экосистему управления данными.

Документация dbt — это сайт документации, генерируемый из вашего проекта dbt, который предоставляет интерфейс для разработчиков и не технических заинтересованных сторон, чтобы понять происхождение данных и бизнес-логику, применяемую к преобразованиям, без необходимости полного доступа к dbt Cloud или Databricks. Это дает вам дополнительные возможности по организации и поиску ваших данных. Вы можете автоматически [создавать и просматривать вашу документацию dbt с помощью dbt Cloud](/docs/collaborate/build-and-view-your-docs), чтобы поддерживать актуальность документации.

Unity Catalog — это единое решение для управления данными для вашего lakehouse. Он предоставляет исследователь данных, который можно использовать для обнаружения наборов данных, которые не были определены в dbt. Исследователь данных также захватывает [происхождение на уровне столбцов](https://docs.databricks.com/data-governance/unity-catalog/data-lineage.html#capture-and-explore-lineage), когда вам нужно проследить происхождение конкретного столбца.

Чтобы максимально использовать оба инструмента, вы можете использовать [конфигурацию persist docs](/reference/resource-configs/persist_docs) для передачи описаний таблиц и столбцов, написанных в dbt, в Unity Catalog, что делает информацию легко доступной для пользователей обоих инструментов. Сохранение описаний в dbt гарантирует, что они находятся под контролем версий и могут быть воспроизведены после удаления таблицы.

### Связанные документы

- [Курс по расширенному развертыванию](https://learn.getdbt.com/courses/advanced-deployment), если вы хотите углубиться в эти темы
- [Автоматическое масштабирование CI: Интеллектуальный Slim CI](https://docs.getdbt.com/blog/intelligent-slim-ci)
- [Запуск задания dbt Cloud в вашем автоматизированном рабочем процессе с помощью Python](https://discourse.getdbt.com/t/triggering-a-dbt-cloud-job-in-your-automated-workflow-with-python/2573)
- [Быстрый старт по Databricks + dbt Cloud](/guides/databricks)
- Свяжитесь с вашей командой по работе с клиентами Databricks, чтобы получить доступ к предварительным функциям на Databricks.

</div>