---
title: Настройка вашего dbt проекта с Databricks
id: set-up-your-databricks-dbt-project
description: "Узнайте больше о настройке вашего dbt проекта с Databricks."
displayText: Настройка вашего dbt проекта с Databricks
hoverSnippet: Узнайте, как настроить ваш dbt проект с Databricks.
icon: 'databricks'
hide_table_of_contents: true
tags: ['Databricks', 'dbt Core','dbt Cloud']
level: 'Intermediate'
recently_updated: true
---

<div style={{maxWidth: '900px'}}>

## Введение

Databricks и dbt Labs сотрудничают, чтобы помочь командам по работе с данными мыслить как команды разработчиков программного обеспечения и быстрее предоставлять надежные данные. Адаптер dbt-databricks позволяет пользователям dbt использовать последние функции Databricks в своем dbt проекте. Сотни клиентов уже используют dbt и Databricks для создания выразительных и надежных конвейеров данных на Lakehouse, создавая активы данных, которые позволяют использовать аналитику, ML и AI в бизнесе.

В этом руководстве мы обсудим, как настроить ваш dbt проект на платформе Databricks Lakehouse, чтобы он масштабировался от небольшой команды до крупной организации.

## Настройка окружений Databricks

Для начала мы будем использовать Unity Catalog от Databricks. Без него мы не смогли бы создать отдельные [окружения](https://docs.getdbt.com/docs/collaborate/environments) для разработки и производства в соответствии с нашими [лучшими практиками](https://docs.getdbt.com/best-practices/how-we-structure/1-guide-overview). Это также позволяет нам гарантировать, что правильные контрольные доступы были применены с использованием SQL. Вам нужно будет использовать адаптер dbt-databricks для его использования (в отличие от адаптера dbt-spark).

Мы создадим два разных *каталога* в Unity Catalog: **dev** и **prod**. Каталог — это контейнер верхнего уровня для *схем* (ранее известных как базы данных в Databricks), которые, в свою очередь, содержат таблицы и представления.

Наш dev каталог будет средой разработки, с которой аналитические инженеры взаимодействуют через свои IDE. Разработчики должны иметь свою собственную песочницу для создания и тестирования объектов без опасения перезаписать или удалить работу коллеги; мы рекомендуем создавать личные схемы для этой цели. С точки зрения разрешений, они должны иметь доступ только к каталогу **dev**.

Только производственные запуски будут иметь доступ к данным в каталоге **prod**. В будущем руководстве мы обсудим каталог **test**, где наша система непрерывной интеграции/непрерывного развертывания (CI/CD) сможет запускать `dbt test`.

Пока давайте упростим и [создадим два каталога](https://docs.databricks.com/sql/language-manual/sql-ref-syntax-ddl-create-catalog.html) либо с помощью Data Explorer, либо в SQL редакторе с помощью этих команд:

```sql
create catalog if not exists dev;
create catalog if not exists prod;
```

Пока вашему разработчику предоставлен доступ на запись в каталог данных dev, нет необходимости заранее создавать песочницы-схемы.

## Настройка сервисных принципалов

Когда аналитический инженер запускает dbt проект из своей IDE, вполне нормально, что полученные запросы выполняются с идентичностью этого пользователя. Однако мы хотим, чтобы производственные запуски выполнялись с идентичностью *сервисного принципала*. Напомним, что сервисный принципал — это учетная запись без привязки к конкретному человеку.

Сервисные принципалы используются для исключения людей из процесса развертывания в производственной среде для удобства и безопасности. Личные идентичности не должны использоваться для создания производственных конвейеров, так как они могут сломаться, если пользователь покинет компанию или изменит свои учетные данные. Также не должно быть произвольных команд, изменяющих производственные данные. Только запланированные задания и код, прошедший тесты CI и код-ревью, должны иметь возможность изменять производственные данные. Если что-то ломается, есть аудируемый след изменений, чтобы найти коренную причину, легко вернуться к последней рабочей версии кода и минимизировать влияние на конечных пользователей.

[Давайте создадим сервисный принципал](https://docs.databricks.com/administration-guide/users-groups/service-principals.html#add-a-service-principal-to-your-databricks-account) в Databricks:

1. Попросите администратора вашего аккаунта Databricks [добавить сервисный принципал](https://docs.databricks.com/administration-guide/users-groups/service-principals.html#add-a-service-principal-to-your-databricks-account) в ваш аккаунт. Имя сервисного принципала должно отличаться от идентификатора пользователя и четко указывать на его назначение (например, dbt_prod_sp).
2. Добавьте сервисный принципал в любые группы, членом которых он должен быть на данный момент. Более подробная информация о разрешениях содержится в нашем руководстве ["Лучшие практики Unity Catalog"](/best-practices/dbt-unity-catalog-best-practices).
3. [Добавьте сервисный принципал в ваше рабочее пространство](https://docs.databricks.com/administration-guide/users-groups/service-principals.html#add-a-service-principal-to-a-workspace) и примените все [необходимые права](https://docs.databricks.com/administration-guide/users-groups/service-principals.html#add-a-service-principal-to-a-workspace-using-the-admin-console), такие как доступ к Databricks SQL и доступ к рабочему пространству.

## Настройка вычислительных ресурсов Databricks

Когда вы запускаете dbt проект, он генерирует SQL, который может выполняться на кластерах общего назначения или SQL складах. Мы настоятельно рекомендуем запускать SQL, сгенерированный dbt, на SQL складе Databricks. Поскольку SQL склады оптимизированы для выполнения SQL запросов, вы можете сэкономить на стоимости за счет меньшего времени работы кластера, необходимого для выполнения запросов. Если вам нужно отладить, у вас также будет доступ к профилю запроса. Мы рекомендуем использовать безсерверный кластер, если вы хотите минимизировать время, затрачиваемое на запуск кластера, и устранить необходимость изменения размеров кластера в зависимости от рабочих процессов.

Давайте [создадим SQL склад Databricks](https://docs.databricks.com/sql/admin/sql-endpoints.html#create-a-sql-warehouse):

1. Нажмите **SQL Warehouses** в боковой панели.
2. Нажмите *Create SQL Warehouse*.
3. Введите имя для склада.
4. Примите настройки склада по умолчанию или измените их.
5. Нажмите *Create*.
6. Настройте разрешения склада, чтобы гарантировать, что наш сервисный принципал и разработчик имеют правильный доступ.

Мы не рассматриваем Python в этом посте, но если вы хотите узнать больше, ознакомьтесь с этими [документами](https://docs.getdbt.com/docs/build/python-models#specific-data-platforms). В зависимости от вашей рабочей нагрузки, вы можете создать более крупный SQL склад для производственных рабочих процессов, имея при этом меньший склад для разработки (если вы не используете безсерверные SQL склады). По мере роста вашего проекта вы можете применить [конфигурации вычислений для моделей](/reference/resource-configs/databricks-configs#specifying-the-compute-for-models).

## Настройка вашего dbt проекта

Теперь, когда компоненты Databricks настроены, мы можем настроить наш dbt проект. Это включает в себя подключение dbt к нашему SQL складу Databricks для выполнения SQL запросов и использование системы контроля версий, такой как GitHub, для хранения нашего кода трансформации.

Если вы мигрируете существующий dbt проект с адаптера dbt-spark на dbt-databricks, следуйте этому [руководству по миграции](/guides/migrate-from-spark-to-databricks), чтобы переключить адаптеры без необходимости обновления учетных данных разработчика и других существующих конфигураций.

Если вы начинаете новый dbt проект, следуйте приведенным ниже шагам. Для более подробного процесса настройки ознакомьтесь с нашим [руководством по быстрому старту.](/guides/databricks)

### Подключение dbt к Databricks

Сначала вам нужно подключить ваш dbt проект к Databricks, чтобы он мог отправлять инструкции по трансформации и создавать объекты в Unity Catalog. Следуйте инструкциям для [dbt Cloud](/guides/databricks?step=4) или [Core](https://docs.getdbt.com/reference/warehouse-setups/databricks-setup), чтобы настроить учетные данные подключения вашего проекта.

Каждый разработчик должен сгенерировать свой Databricks PAT и использовать токен в своих учетных данных для разработки. Они также укажут уникальную схему разработчика, которая будет хранить таблицы и представления, сгенерированные dbt запусками из их IDE. Это обеспечивает изолированные среды разработчиков и гарантирует, что доступ к данным соответствует назначению.

Давайте сгенерируем [персональный токен доступа Databricks (PAT)](https://docs.databricks.com/sql/user/security/personal-access-tokens.html) для разработки:

1. В Databricks нажмите на ваше имя пользователя в верхней панели и выберите User Settings в выпадающем меню.
2. На вкладке Access token нажмите Generate new token.
3. Нажмите Generate.
4. Скопируйте отображаемый токен и нажмите Done. (не потеряйте его!)

Для ваших учетных данных разработки/profiles.yml:

1. Установите ваш каталог по умолчанию на dev.
2. Ваша схема разработчика должна быть названа в честь вас. Мы рекомендуем dbt_&lt;первая_буква_имени&gt;&lt;фамилия&gt;.

Во время первого вызова `dbt run`, dbt создаст схему разработчика, если она еще не существует в каталоге dev.

## Определение среды развертывания dbt

Нам нужно дать dbt способ развертывания кода вне сред разработки. Для этого мы будем использовать [окружения](https://docs.getdbt.com/docs/collaborate/environments) dbt, чтобы определить производственные цели, с которыми будут взаимодействовать конечные пользователи.

Core проекты могут использовать [цели в профилях](https://docs.getdbt.com/docs/core/connection-profiles#understanding-targets-in-profiles) для разделения окружений. [Окружения dbt Cloud](https://docs.getdbt.com/docs/cloud/develop-in-the-cloud#set-up-and-access-the-cloud-ide) позволяют вам определять окружения через интерфейс и [планировать задания](/guides/databricks#create-and-run-a-job) для конкретных окружений.

Давайте настроим нашу среду развертывания:

1. Следуйте инструкциям Databricks, чтобы [настроить токен вашего сервисного принципала](https://docs.databricks.com/dev-tools/service-principals.html#use-curl-or-postman). Обратите внимание, что `lifetime_seconds` определит, как долго эти учетные данные будут оставаться действительными. Вы должны использовать большое число здесь, чтобы избежать частого регенерирования токенов и сбоев производственных заданий.
2. Теперь давайте вернемся к dbt Cloud, чтобы заполнить поля окружения. Нажмите на окружения в интерфейсе dbt Cloud или определите новую цель в вашем profiles.yml.
3. Установите *каталог* производственной среды на **prod** каталог, созданный выше. Предоставьте [сервисный токен](https://docs.databricks.com/administration-guide/users-groups/service-principals.html#manage-access-tokens-for-a-service-principal) для вашего **prod** сервисного принципала и установите его как *токен* в учетных данных развертывания вашей производственной среды.
4. Установите схему по умолчанию для вашей prod среды. Это может быть переопределено [пользовательскими схемами](https://docs.getdbt.com/docs/build/custom-schemas#what-is-a-custom-schema), если вам нужно использовать более одной.
5. Предоставьте токен вашего сервисного принципала.

## Подключение dbt к вашему git репозиторию

Далее вам понадобится место для хранения и контроля версий вашего кода, которое позволит вам сотрудничать с коллегами. Подключите ваш dbt проект к git репозиторию с помощью [dbt Cloud](/guides/databricks#set-up-a-dbt-cloud-managed-repository). [Core](/guides/manual-install#create-a-repository) проекты будут использовать git CLI.

### Следующие шаги

Теперь, когда ваш проект настроен, вы можете начать трансформировать ваши данные в Databricks с помощью dbt. Чтобы помочь вам масштабироваться эффективно, мы рекомендуем следовать нашим лучшим практикам, начиная с [лучших практик Unity Catalog](/best-practices/dbt-unity-catalog-best-practices), затем вы можете [оптимизировать модели dbt на Databricks](/guides/optimize-dbt-models-on-databricks).

</div>