---
title: "Конфигурации AWS Glue"
description: "Конфигурации AWS Glue - Прочитайте это подробное руководство, чтобы узнать о конфигурациях в dbt."
id: "glue-configs"
---

## Конфигурирование таблиц

При материализации модели как `table` вы можете включить несколько дополнительных конфигураций, специфичных для плагина dbt-glue, помимо [конфигурации модели Apache Spark](/reference/resource-configs/spark-configs#configuring-tables).

| Опция  | Описание                                        | Обязательно?               | Пример                  |
|---------|----------------------------------------------------|-------------------------|--------------------------|
| custom_location  | По умолчанию адаптер будет хранить ваши данные по следующему пути: `location path`/`database`/`table`. Если вы не хотите следовать этому поведению по умолчанию, вы можете использовать этот параметр, чтобы установить собственное местоположение на S3 | Нет | `s3://mycustombucket/mycustompath`              |

## Инкрементальные модели

dbt стремится предложить полезные, интуитивно понятные абстракции моделирования с помощью своих встроенных конфигураций и материализаций.

По этой причине плагин dbt-glue сильно полагается на конфигурацию [`incremental_strategy`](/docs/build/incremental-strategy). Эта конфигурация указывает инкрементальной материализации, как строить модели в запусках после первого. Она может быть установлена на одно из трех значений:
 - **`append`** (по умолчанию): Вставить новые записи без обновления или перезаписи существующих данных.
 - **`insert_overwrite`**: Если указано `partition_by`, перезаписать разделы в таблице новыми данными. Если `partition_by` не указано, перезаписать всю таблицу новыми данными.
 - **`merge`** (только Apache Hudi): Сопоставить записи на основе `unique_key`; обновить старые записи, вставить новые. (Если `unique_key` не указан, все новые данные вставляются, аналогично `append`.)

Каждая из этих стратегий имеет свои плюсы и минусы, которые мы обсудим ниже. Как и с любой конфигурацией модели, `incremental_strategy` может быть указана в `dbt_project.yml` или в блоке `config()` файла модели.

**Примечания:**
Стратегия по умолчанию - **`insert_overwrite`**.

### Стратегия `append`

Следуя стратегии `append`, dbt выполнит оператор `insert into` со всеми новыми данными. Привлекательность этой стратегии заключается в том, что она проста и функциональна на всех платформах, типах файлов, методах подключения и версиях Apache Spark. Однако эта стратегия _не может_ обновлять, перезаписывать или удалять существующие данные, поэтому она, вероятно, вставит дублирующие записи для многих источников данных.

### Стратегия `insert_overwrite`

Эта стратегия наиболее эффективна, когда указано условие `partition_by` в конфигурации вашей модели. dbt выполнит [атомарный оператор `insert overwrite`](https://spark.apache.org/docs/3.1.2/sql-ref-syntax-dml-insert-overwrite-table.html), который динамически заменяет все разделы, включенные в ваш запрос. Обязательно повторно выберите _все_ соответствующие данные для раздела при использовании этой инкрементальной стратегии.

Если `partition_by` не указано, то стратегия `insert_overwrite` атомарно заменит все содержимое таблицы, перезаписывая все существующие данные только новыми записями. Однако схема столбцов таблицы останется прежней. Это может быть желаемо в некоторых ограниченных обстоятельствах, поскольку минимизирует время простоя во время перезаписи содержимого таблицы. Операция сопоставима с выполнением `truncate` + `insert` в других базах данных. Для атомарной замены таблиц в формате Delta используйте материализацию `table` (которая выполняет `create or replace`) вместо этого.

Указание `insert_overwrite` в качестве инкрементальной стратегии является необязательным, так как это стратегия по умолчанию, используемая, когда ничего не указано.

### Стратегия `merge`

**Примечания по использованию:** Инкрементальная стратегия `merge` требует:
- `file_format: hudi`
- AWS Glue runtime 2 с библиотеками hudi в качестве дополнительных jar-файлов

Вы можете добавить библиотеки hudi в качестве дополнительных jar-файлов в classpath, используя опции extra_jars в вашем profiles.yml.
Вот пример:
```yml
extra_jars: "s3://dbt-glue-hudi/Dependencies/hudi-spark.jar,s3://dbt-glue-hudi/Dependencies/spark-avro_2.11-2.4.4.jar"
```

dbt выполнит [атомарный оператор `merge`](https://hudi.apache.org/docs/writing_data#spark-datasource-writer), который выглядит почти идентично поведению по умолчанию для слияния в Snowflake и BigQuery. Если указан `unique_key` (рекомендуется), dbt обновит старые записи значениями из новых записей, которые совпадают по ключевому столбцу. Если `unique_key` не указан, dbt не будет учитывать критерии совпадения и просто вставит все новые записи (аналогично стратегии `append`).

## Сохранение описаний моделей

Сохранение документации на уровне отношений унаследовано от dbt-spark, для получения дополнительной информации смотрите [конфигурацию модели Apache Spark](/reference/resource-configs/spark-configs#persisting-model-descriptions).

## Всегда `schema`, никогда `database`

Этот раздел также унаследован от dbt-spark, для получения дополнительной информации смотрите [конфигурацию модели Apache Spark](/reference/resource-configs/spark-configs#always-schema-never-database).