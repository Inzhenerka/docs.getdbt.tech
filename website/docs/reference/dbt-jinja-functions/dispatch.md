---
sidebar_label: "dispatch"
title: "О конфигурации dispatch"
id: "dispatch"
description: "dbt расширяет функциональность на различных платформах данных с помощью множественной диспетчеризации."
---

dbt может расширять функциональность на [Поддерживаемых платформах данных](/docs/supported-data-platforms) с помощью системы [множественной диспетчеризации](https://en.wikipedia.org/wiki/Multiple_dispatch). Поскольку синтаксис SQL, типы данных и поддержка <Term id="ddl" />/<Term id="dml" /> различаются между адаптерами, dbt может определять и вызывать универсальные функциональные макросы, а затем "диспетчеризовать" этот макрос к соответствующей реализации для текущего адаптера.

## Синтаксис

__Аргументы__:

  * `macro_name` [обязательный]: Имя макроса для диспетчеризации. Должно быть строковым литералом.
  * `macro_namespace` [необязательный]: Пространство имен (пакет) макроса для диспетчеризации. Должно быть строковым литералом.

__Использование__:

```sql
{% macro my_macro(arg1, arg2) -%}
  {{ return(adapter.dispatch('my_macro')(arg1, arg2)) }}
{%- endmacro %}
```

dbt использует два критерия при поиске подходящего макроса:
- Префикс адаптера
- Пространство имен (пакет)

**Префикс адаптера:** Макросы, специфичные для адаптера, имеют префикс с именем адаптера в нижнем регистре и двумя подчеркиваниями. Для макроса с именем `my_macro` dbt будет искать:
* Postgres: `postgres__my_macro`
* Redshift: `redshift__my_macro`
* Snowflake: `snowflake__my_macro`
* BigQuery: `bigquery__my_macro`
* OtherAdapter: `otheradapter__my_macro`
* _по умолчанию:_ `default__my_macro`

Если dbt не находит реализацию, специфичную для адаптера, он будет диспетчеризовать к реализации по умолчанию.

**Пространство имен:** Обычно dbt будет искать реализации в корневом проекте и внутренних проектах (например, `dbt`, `dbt_postgres`). Если аргумент `macro_namespace` предоставлен, он будет искать в указанном пространстве имен (пакете) подходящие реализации. Также возможно динамически направлять поиск пространства имен, определив [`dispatch` конфигурацию проекта](/reference/project-configs/dispatch-config); смотрите примеры ниже для подробностей.

## Примеры

### Простой пример

Предположим, я хочу определить макрос `concat`, который компилируется в SQL-функцию `concat()` как его
поведение по умолчанию. Однако на Redshift и Snowflake я хочу использовать оператор `||`.

<File name='macros/concat.sql'>

```sql
{% macro concat(fields) -%}
    {{ return(adapter.dispatch('concat')(fields)) }}
{%- endmacro %}


{% macro default__concat(fields) -%}
    concat({{ fields|join(', ') }})
{%- endmacro %}


{% macro redshift__concat(fields) %}
    {{ fields|join(' || ') }}
{% endmacro %}


{% macro snowflake__concat(fields) %}
    {{ fields|join(' || ') }}
{% endmacro %}
```

</File>

Верхний макрос `concat` следует специальной, жесткой формуле: он называется по "основному имени" макроса, `concat`, что и будет использоваться для вызова макроса в других местах. Он принимает один аргумент, названный `fields`. Единственная функция этого макроса — диспетчеризация, то есть поиск и возврат — с использованием основного имени макроса (`concat`) в качестве поискового термина. Он также хочет передать все ключевые аргументы, которые были переданы ему, в его окончательную реализацию. В данном случае есть только один аргумент, названный `fields`.

Ниже этого макроса я определил три возможные реализации макроса `concat`: одну для Redshift, одну для Snowflake и одну для использования по умолчанию на всех других адаптерах. В зависимости от адаптера, с которым я работаю, один из этих макросов будет выбран, ему будут переданы указанные аргументы в качестве входных данных, он будет работать с этими аргументами и вернет результат в исходный диспетчеризующий макрос.

### Более сложный пример

Я нашел существующую реализацию макроса `concat` в пакете dbt-utils. Однако я хочу переопределить его реализацию макроса `concat` на Redshift в частности. Во всех других случаях, включая реализацию по умолчанию, я вполне доволен тем, чтобы вернуться к реализациям, определенным в `dbt_utils.concat`.

<File name='macros/concat.sql'>

```sql
{% macro concat(fields) -%}
    {{ return(adapter.dispatch('concat')(fields)) }}
{%- endmacro %}

{% macro default__concat(fields) -%}
    {{ return(dbt_utils.concat(fields)) }}
{%- endmacro %}

{% macro redshift__concat(fields) %}
    {% for field in fields %}
        nullif({{ field }},'') {{ ' || ' if not loop.last }}
    {% endfor %}
{% endmacro %}
```

</File>

Если я работаю на Redshift, dbt будет использовать мою версию; если я работаю на любой другой базе данных, макрос `concat()` будет ссылаться на версию, определенную в `dbt_utils`.

## Для поддерживающих пакетов

Диспетчеризованные макросы из [пакетов](/docs/build/packages) _должны_ предоставлять аргумент `macro_namespace`, так как это объявляет пространство имен (пакет), в котором он планирует искать кандидатов. Чаще всего это то же самое, что и имя вашего пакета, например, `dbt_utils`. (Возможно, хотя и редко желательно, определить диспетчеризованный макрос _не_ в пакете `dbt_utils` и диспетчеризовать его в пространство имен `dbt_utils`.)

Здесь у нас есть определение макроса `dbt_utils.concat`, который указывает как `macro_name`, так и `macro_namespace` для диспетчеризации:

```sql
{% macro concat(fields) -%}
  {{ return(adapter.dispatch('concat', 'dbt_utils')(fields)) }}
{%- endmacro %}
```

### Переопределение макросов пакета

Следуя второму примеру выше: Каждый раз, когда я вызываю свою версию макроса `concat` в своем проекте, он будет использовать мою специальную версию обработки null на Redshift. Но версия макроса `concat` _внутри_ пакета dbt-utils не будет использовать мою версию.

Почему это важно? Другие макросы в dbt-utils, такие как `surrogate_key`, вызывают макрос `dbt_utils.concat` напрямую. Что если я хочу, чтобы `dbt_utils.surrogate_key` использовал _мою_ версию `concat`, включая мою пользовательскую логику на Redshift?

Как пользователь, я могу достичь этого через [конфигурацию `dispatch` на уровне проекта](/reference/project-configs/dispatch-config). Когда dbt идет диспетчеризовать `dbt_utils.concat`, он знает из аргумента `macro_namespace`, что нужно искать в пространстве имен `dbt_utils`. Конфигурация ниже определяет динамическое направление для этого пространства имен, указывая dbt искать через упорядоченную последовательность пакетов, а не только в пакете `dbt_utils`.

<File name='dbt_project.yml'>

```yml
dispatch:
  - macro_namespace: dbt_utils
    search_order: ['my_project', 'dbt_utils']
```

</File>

Обратите внимание, что эта конфигурация _должна_ быть указана в корневом `dbt_project.yml` пользователя. dbt проигнорирует любые конфигурации `dispatch`, определенные в файлах проектов установленных пакетов.

Префиксы адаптеров все еще имеют значение: dbt будет искать только реализации, которые совместимы с текущим адаптером. Но dbt будет придавать приоритет специфичности пакета над специфичностью адаптера. Если я вызываю макрос `concat`, работая на Postgres, с конфигурацией выше, dbt будет искать следующие макросы в порядке:

1. `my_project.postgres__concat` (не найден)
2. `my_project.default__concat` (не найден)
3. `dbt_utils.postgres__concat` (не найден)
4. `dbt_utils.default__concat` (найден! используйте этот)

Как человек, устанавливающий пакет, эта функциональность позволяет мне изменить поведение другого, более сложного макроса (`dbt_utils.surrogate_key`), переопределив один из его модульных компонентов.

Как поддерживающий пакет, эта функциональность позволяет пользователям моего пакета расширять, переопределять или изменять поведение по умолчанию, не прибегая к форку исходного кода пакета.

### Переопределение глобальных макросов

:::tip
Некоторые функции, такие как [`ref`](/reference/dbt-jinja-functions/ref), [`source`](/reference/dbt-jinja-functions/source) и [`config`](/reference/dbt-jinja-functions/config), не могут быть переопределены с помощью пакета, использующего конфигурацию диспетчеризации. Это связано с тем, что `ref`, `source` и `config` являются свойствами контекста внутри dbt и не диспетчеризуются как глобальные макросы. Обратитесь к [этому обсуждению на GitHub](https://github.com/dbt-labs/dbt-core/issues/4491#issuecomment-994709916) для получения дополнительной информации.
:::

Я поддерживаю внутренний утилитарный пакет в своей организации, названный `my_org_dbt_helpers`. Я использую этот пакет для переопределения встроенных макросов dbt от имени всех моих коллег, использующих dbt, которые работают над рядом проектов dbt.

Мой пакет может определять пользовательские версии любых диспетчеризованных глобальных макросов, которые я выберу, от `generate_schema_name` до `test_unique`. Я могу определить новую версию по умолчанию этого макроса (например, `default__generate_schema_name`), или пользовательские версии для конкретных <Term id="data-warehouse" /> адаптеров (например, `spark__generate_schema_name`).

Каждому корневому проекту, устанавливающему мой пакет, просто нужно включить [конфигурацию `dispatch` на уровне проекта](/reference/project-configs/dispatch-config), которая ищет мой пакет перед `dbt` для глобального пространства имен `dbt`:

<File name='dbt_project.yml'>

```yml
dispatch:
  - macro_namespace: dbt
    search_order: ['my_project', 'my_org_dbt_helpers', 'dbt']
```

</File>

### Управление различными глобальными переопределениями в разных пакетах

Вы можете переопределять глобальные поведения различными способами для каждого проекта, установленного в качестве пакета. Это верно для всех глобальных макросов: `generate_schema_name`, `create_table_as` и т.д. При парсинге или выполнении ресурса, определенного в пакете, определение глобального макроса в этом пакете имеет приоритет над определением в корневом проекте, поскольку оно более специфично для этих ресурсов.

Сочетая переопределения на уровне пакета и `dispatch`, можно достичь трех различных паттернов:

1. **Пакет всегда выигрывает** &mdash; Как разработчик моделей dbt в проекте, который будет развернут в другом месте в качестве пакета, вы хотите полностью контролировать макросы, используемые для определения и материализации ваших моделей. Ваши макросы всегда должны иметь приоритет для ваших моделей, и не должно быть никакого способа их переопределить.

    - _Механизм:_ Каждый проект/пакет полностью переопределяет макрос по его имени, например, `generate_schema_name` или `create_table_as`. Не используйте диспетчеризацию.

2. **Условное применение (выигрывает корневой проект)** &mdash; Как поддерживающий один проект dbt в сети нескольких, ваша команда хочет условного применения этих правил. Когда вы запускаете свой проект самостоятельно (в разработке), вы хотите применить пользовательское поведение; но когда он установлен в качестве пакета и развернут вместе с несколькими другими проектами (в производстве), вы хотите, чтобы правила корневого проекта применялись.

    - _Механизм:_ Каждый пакет реализует свое "локальное" переопределение, регистрируя кандидата для диспетчеризации с префиксом адаптера, например, `default__generate_schema_name` или `default__create_table_as`. Корневой проект может затем зарегистрировать свой собственный кандидат для диспетчеризации (`default__generate_schema_name`), выиграв порядок поиска по умолчанию или явно переопределив макрос по имени (`generate_schema_name`).

3. **Одни и те же правила везде и всегда** &mdash; Как член команды платформы данных, ответственный за согласованность между командами в вашей организации, вы хотите создать "пакет макросов", который каждая команда может установить и использовать.

    - _Механизм:_ Создайте отдельный пакет только с кандидатами макросов, например, `default__generate_schema_name` или `default__create_table_as`. Добавьте [конфигурацию `dispatch` на уровне проекта](/reference/project-configs/dispatch-config) в каждом `dbt_project.yml` проекта.

## Для поддерживающих адаптеров

Большинство пакетов изначально были разработаны для работы с четырьмя оригинальными адаптерами dbt. Используя макрос `dispatch` и конфигурацию проекта, можно "подстроить" существующие пакеты для работы с другими адаптерами с помощью пакетов совместимости третьих сторон.

Например, если я хочу использовать `dbt_utils.concat` на Apache Spark, я могу установить пакет совместимости `spark-utils` вместе с `dbt-utils`:

<File name='packages.yml'>

```yml
packages:
  - package: dbt-labs/dbt_utils
    version: ...
  - package: dbt-labs/spark_utils
    version: ...
```

</File>

Затем я включаю `spark_utils` в порядок поиска для диспетчеризованных макросов в пространстве имен `dbt_utils`. (Я все еще включаю свой собственный проект первым, на случай, если я захочу переопределить какие-либо макросы с помощью своей пользовательской логики.)

<File name='dbt_project.yml'>

```yml
dispatch:
  - macro_namespace: dbt_utils
    search_order: ['my_project', 'spark_utils', 'dbt_utils']
```

</File>

При диспетчеризации `dbt_utils.concat` dbt будет искать:

1. `my_project.spark__concat` (не найден)
2. `my_project.default__concat` (не найден)
3. `spark_utils.spark__concat` (найден! используйте этот)
4. `spark_utils.default__concat`
5. `dbt_utils.postgres__concat`
6. `dbt_utils.default__concat`

Как поддерживающий пакет совместимости, мне нужно только переопределить основные строительные блоки макросов, которые инкапсулируют низкоуровневые синтаксические различия. Переопределив низкоуровневые макросы, такие как `spark__dateadd` и `spark__datediff`, пакет `spark_utils` предоставляет доступ к более сложным макросам (`dbt_utils.date_spine`) "бесплатно".

Как пользователь `dbt-spark`, установив `dbt_utils` и `spark_utils` вместе, я получаю не только доступ к более высоким утилитарным макросам. Я даже могу установить и использовать пакеты без логики, специфичной для Spark, и которые никогда не тестировались на Spark, если они полагаются на макросы `dbt_utils` для совместимости между адаптерами.

### Наследование адаптера

Некоторые адаптеры "наследуют" от других адаптеров (например, `dbt-postgres` &rarr; `dbt-redshift`, и `dbt-spark` &rarr; `dbt-databricks`). Если используется дочерний адаптер, dbt также будет включать реализации родительского адаптера в свой порядок поиска. Вместо того чтобы просто искать `redshift__` и возвращаться к `default__`, dbt будет искать `redshift__`, `postgres__` и `default__` в этом порядке.

Дочерние адаптеры, как правило, имеют очень похожий синтаксис SQL на своих родителей, поэтому это позволяет им пропустить переопределение макроса, который уже был переопределен родительским адаптером.

Следуя примеру выше с `dbt_utils.concat`, полный порядок поиска на Redshift на самом деле будет:

1. `my_project.redshift__concat`
2. `my_project.postgres__concat`
3. `my_project.default__concat`
4. `dbt_utils.redshift__concat`
5. `dbt_utils.postgres__concat`
6. `dbt_utils.default__concat`

В редких случаях дочерний адаптер может предпочесть реализацию по умолчанию своей реализации, специфичной для родительского адаптера. В этом случае дочерний адаптер должен определить специфичный для адаптера макрос, который вызывает реализацию по умолчанию. Например, синтаксис PostgreSQL для добавления дат должен работать и на Redshift, но я могу предпочесть простоту `dateadd`:

```sql
{% macro dateadd(datepart, interval, from_date_or_timestamp) %}
    {{ return(adapter.dispatch('dateadd')(datepart, interval, from_date_or_timestamp)) }}
{% endmacro %}

{% macro default__dateadd(datepart, interval, from_date_or_timestamp) %}
    dateadd({{ datepart }}, {{ interval }}, {{ from_date_or_timestamp }})
{% endmacro %}

{% macro postgres__dateadd(datepart, interval, from_date_or_timestamp) %}
    {{ from_date_or_timestamp }} + ((interval '1 {{ datepart }}') * ({{ interval }}))
{% endmacro %}

{# Используйте синтаксис по умолчанию вместо синтаксиса postgres #}
{% macro redshift__dateadd(datepart, interval, from_date_or_timestamp) %}
    {{ return(default__dateadd(datepart, interval, from_date_or_timestamp)) }}
{% endmacro %}
```

## Часто задаваемые вопросы

<FAQ path="Troubleshooting/dispatch-could-not-find-package" />